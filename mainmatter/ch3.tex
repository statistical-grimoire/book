%\chapter{The Basics of Loading and Manipulating Data}
\chapter{The Invocation and Metamorphosis of Data}

\IMFellEnglish
\lettrine[lines=5, realheight]{K}{NOWLEDGE} is power as they say, but dataâ€”data is something else entirely. It is the ghost in the machine, the thing lurking beneath the surface, waiting for you to look too close. Heed this warning: The data frame, and its accursed successor the tibble, are your most loyal servants \ldots{} and your most treacherous foes. Treat them with reverence, for a single misstep may awaken errors best left entombed.

\normalfont

Chapter 1 had stated that \glspl{data frame} are essential for keeping a host of related information stored in a well organized manner that is easy to manipulate. When printed to the console, data frames present information in a familiar spreadsheet-like structure that can be created, subset, and altered in various ways (see section \ref{sec:data_frames} for details). Moreover, in chapter 1, we saw how a data frame can be constructed by manually entering values with R code. And, for all but the smallest of data sets, this method, while simple, is both time-consuming and highly prone to error. A better strategy is to take an existing file of information and import that directly into R as a data frame or, depending on the nature of the data and what needs to be done with it, as a list, matrix, array, or table. Though, a data frame is usually going to be the optimal choice and will be the primary focus of this chapter.

Data can come in all manner of different layouts and file formats and, in this respect, R has the ability to handle pretty much any scenario that might arise. This chapter will be working under the assumption that the kind of data that you need to work with is in a conventional ``spreadsheet-style'' of format. That is to say, like the \R{msleep} data used in Chapter 2, there are sets of rows and columns, with each cell containing just a single value.

\section{Spreadsheet Software}
\label{sec:spreadsheet_soft}

Given the ubiquity of spreadsheet software, it is important to discuss its use and why R offers a preferable alternative for data analysis. Most spreadsheet applications have their own specific file type that is tailored to its unique purpose and platform. For instance, \textit{Microsoft's Excel} spreadsheet application has its own proprietary format called the \texttt{.xlsx} file format. The awful stock spreadsheet application on Macintosh computers, called \textit{Numbers}, uses the \texttt{.NUMBERS} file format. And if you use an open-source spreadsheet software like \textit{Libre Office's Calc} application, you may be familiar with the \texttt{.ods} file format.

As everyone who is reading this doubtlessly appreciates, spreadsheet applications like Microsoft's Excel, Numbers, Libre Office's Calc, etc., do more than just structure your data in a big table.  They allow you to do things like perform calculations, adjust cell colours, add images, insert comments, etc. And all of this is saved, in one form or another, as information inside the specific file associated with that software. These features make applications like Microsoft's Excel, for instance, a great tool for basic tasks like balancing the household budget.  However, for serious data analysis that requires the use of large data sets and complex or heavy calculations, this kind of software is going to be more of a hindrance than a help. Incorporating all those layers of additional functionality is going to boost file sizes, inflate load times, limit the amount of information the spreadsheet can hold, and increase the chance of a glitch occurring. Additionally, and most importantly, both the analyses and the data are all contained within the same file, which makes it very easy to irrevocably damage your original data set, often without even realizing it. The fact is, we should care about analysing our data efficiently and safely, not making it look pretty in what amounts to a fancy table, and this is one of the key benefits of using R.

From the point of view of R, a spreadsheet is just a way of displaying the raw information to be analysed and nothing more. The analysis of that information is what R does. Technically then, we should not be referring to something as a ``spreadsheet file,'' but rather a ``data file.'' The spreadsheet aspect of all of this is more about how the data is structured for our viewing as humans. However, data does not necessarily need to be viewed as a spreadsheet - it can be viewed in all kinds of different ways. It is just that a spreadsheet is usually the most convenient and intuitive way to view it and talk about it.

\section{Using an Ethical File Format}
\label{sec:ethical_file}

As noted above, there are a variety of different spreadsheet file types data could be formatted as (\texttt{.xlsx}, \texttt{.ods}, \texttt{.wks}, etc.). To remain consistent with open-science principles \parencite{UNESCO_open_sci}, best practice dictates that you work with your data in a file format that is both universally recognized across applications and will also stand the test of time in terms of compatibility.  In other words, we want to (ideally) work with a file format that has no immediate risk of becoming obsolete and can be read by multiple computers on multiple platforms without forcing the user to pay for some proprietary application. Along these lines, the most widely used and recognized format is the \texttt{.csv} file format.

\section{The .CSV Format}

``CSV'' stands for ``comma separated values.'' It gets its name from the fact that it is, quite literally, nothing more than a generic text document that uses commas to denote a tabular (spreadsheet) structure in the data.\footnote{``Tabular'' and ``spreadsheet'' mean the same thing here.} This is easiest to see with an example. The GitHub repository for this book contains a file called \R{skull\_cap\_partial\_wide.csv}. It is located in the \R{./data} directory at the following URL:

\begin{center}
\url{https://github.com/statistical-grimoire/book/blob/main/data/Egyptian-skulls}
\end{center}

\noindent
This data represents a subset of a much larger dataset,\footnote{\R{Thomson\_Randall-MacIver\_1905.csv}} containing estimated cranial capacities in cubic centimetres for 1,449 ancient Egyptian skulls.\footnote{This is not necessarily a statistic anyone should care about, but Ancient Egypt is really cool and skulls are metal AF. Also, for any Americans reading this, 1 centimetre is equal to 1.181 barleycorns.} These skulls span numerous historical periods, ranging from Egypt's early predynastic era to its Roman occupation.

Upon opening the file on GitHub,\footnote{\url{https://github.com/statistical-grimoire/book/blob/main/data/Egyptian-skulls/skull_cap_partial_wide.csv}} the contents appear in a standard tabular format, resembling a typical spreadsheet (see Table \ref{tab:skulls_wide} for an example displaying the first ten rows). With the exception of the columns labelled \R{sex} and \R{predynastic}, each column header corresponds to the starting year of an approximate date range, as reported by the original authors. The prefix \textit{c} denotes \textit{circa} (``approximately''), followed by a year and the abbreviation \textit{BC} (``Before Christ''), reflecting the historical dating conventions employed by the original authors. A more contemporary and inclusive alternative would be \textit{BCE} (``Before Common Era''). The term \textit{predynastic} refers to periods preceding the earliest recorded Egyptian dynasties, which, at the time of \citeauthor{Thomson1905}'s (\citeyear{Thomson1905}) research, were not yet clearly established or reliably dated.

\vspace{1em}

\input{tables/ch-3/skulls_wide.tex}

While GitHub nicely formats the file as a spreadsheet for viewing, the actual raw data consists of nothing more than a basic text document that separates individual values with a comma. We can see this more clearly if we click the button on GitHub labelled ``Raw'' which will present the file in its unaltered (i.e., raw) text format. The first 10 rows can be seen below:

\vspace{1em}

\begin{listing}[H]
\begin{raw}
sex,predynastic,c4800BC,c4200BC,c4000BC,c3700BC,c3500BC,c2780BC,c1590BC,c378BC,c331BC
Male,1370,1410,1320,1445,,1395,1425,1440,1310,1450
Male,1250,1445,1565,1540,,1420,1505,1355,1395,1460
Male,1430,1440,1600,1565,,1380,1360,1490,1360,1360
Male,1350,1340,1460,1710,,1260,1385,1425,1485,1410
Male,1130,1460,1520,1690,,1285,1350,1380,1365,1215
Male,1670,1290,1440,1775,,1505,1440,1490,1220,1320
Male,1195,1290,1740,1390,,1230,1400,1385,1195,1550
Male,1500,1385,1410,1620,,1250,1255,1270,1410,1320
Male,1325,1290,1510,1500,,1315,1450,1585,1370,1460
Male,1480,1565,1550,1255,,1360,1310,1330,1365,1560
\end{raw}
\caption*{Example of the \R{skull\_cap\_partial\_wide.csv} data file displayed in its raw text format. Only the first ten rows are shown.}
\end{listing}

Comparing the two versions it can readily be seen how the commas are functioning. They separate individual columns and each new line represents a new row in the spreadsheet. This not only makes it easy to read \texttt{.csv} files within a basic text editor, but create them as well. Just save (or rename) the text document with a \texttt{.csv} file extension (which you may need to configure your computer to display). Alternatively, if you have a good spreadsheet software on your computer, it will have the ability to \textit{``Save As''} a \texttt{.csv} file or \textit{``Export''} to one. For instance, the save menu of Microsoft Excel will present the user with a drop down list of potential file types it can save as and (as of writing this) has four different versions of \texttt{.csv} files (the best option is the one labelled ``UTF-8 Comma delimited''). By contrast the Numbers application on a Mac will not permit a spreadsheet to save as anything other than a \texttt{.NUMBERS} file, but will allow you to export your saved spreadsheet as a \texttt{.csv}. Just select \textit{File $\rightarrow$ Export To $\rightarrow$ CSV.}

\section{Delimiters}

In the case of the \R{skull\_cap\_partial\_wide.csv} file, the comma is functioning as a \gls{delimiter}; which is to say it is a character that defines the limits of (i.e., it ``delimits'') individual values. Commas are not the only characters that can be used to delimit, any character can technically be used. Other common delimiters include semicolons (;) and tab-key spaces. Semicolons are often used when the data is logged with commas representing decimal points instead of periods (e.g., 13.666 = 13,666), which is a frequent practice in many countries. Oddly, when a delimited file uses semicolons, it is still often given a \texttt{.csv} file extension despite it being a completely different character.  In R, to avoid confusion, the convention is to refer to these semicolon delimited files as \R{csv2} files in function names (e.g., \R{write\_csv()} would use a comma to delimit whereas \R{write\_csv2()} would use a semicolon).

Tab spaces (i.e., pressing ``tab'' on your keyboard), are also frequently employed as a delimiter, but these are usually denoted as \texttt{.tsv} files (i.e., tab separated values). In fact, the name for the keyboard key ``tab'' comes from the the verb ``tabulate'' because the key facilitated easier generation of tables when working on a typewriter. Prior to the tab key's development, the space bar had to be repeatedly pressed to advance the typewriter's carriage to align columns appropriately. 

If you were to save the \R{skull\_cap\_partial\_wide.csv} data set as a \texttt{.tsv} file and open it within a generic text editor, you would see something very similar to the following ...

\vspace{1em}

\begin{listing}[H]
\begin{raw}
sex	predynastic	c4800BC	c4200BC	c4000BC	c3700BC	c3500BC	c2780BC
Male	1370	1410	1320	1445	NA	1395	1425	1440	1310	1450
Male	1250	1445	1565	1540	NA	1420	1505	1355	1395	1460
Male	1430	1440	1600	1565	NA	1380	1360	1490	1360	1360
Male	1350	1340	1460	1710	NA	1260	1385	1425	1485	1410
Male	1130	1460	1520	1690	NA	1285	1350	1380	1365	1215
Male	1670	1290	1440	1775	NA	1505	1440	1490	1220	1320
Male	1195	1290	1740	1390	NA	1230	1400	1385	1195	1550
Male	1500	1385	1410	1620	NA	1250	1255	1270	1410	1320
Male	1325	1290	1510	1500	NA	1315	1450	1585	1370	1460
Male	1480	1565	1550	1255	NA	1360	1310	1330	1365	1560
\end{raw}
\caption*{Excerpt of the \R{skull\_cap\_partial\_wide.csv} file displayed in raw text format as if it were a \texttt{.tsv}. Only the first 10 rows are shown; the last three column headers (\texttt{c1590BC}, \texttt{c378BC}, and \texttt{c331BC}) are omitted for space.}
\end{listing}

\noindent Notice that the tabular separation gives the file a much more grid-like aesthetic that is easier to read. Incorporating spaces into the text file can be used to further refine the alignment.

\section{Reading a CSV File into R}

Now that we have a good sense of what a \texttt{.csv} file is, we should discuss how to load it into R as a data frame object so we can conduct our analyses. To begin with, you should download \R{skull\_cap\_partial\_wide.csv} from the aforementioned GitHub repo by simply clicking the ``down arrow'' icon labelled ``\textit{Download raw file.}'' Once downloaded, simply place the file inside your working directory.\footnote{If you are unsure what a ``working directory'' is see section \ref{sec:dir}} Depending on the browser you are using you may have to hunt around for the download option. For instance, if you are using Safari, you may have to select ``more file actions.''

With the file in its appropriate location you can simply run the function \R{read\_csv()} and give it the full name (with extension) of your file as a character string. This will create a data frame object in R. However, \R{read\_csv()} is a function that belongs to the \textit{readr} package which is part of the \textit{tidyverse}, so if you do not have the \textit{tidyverse} loaded, this will not work. In order to easily call our loaded data, we will assign it the name \R{skulls}.

\begin{inR}
library(tidyverse)
skulls <- read_csv("skull_cap_partial_wide.csv")
\end{inR}

\begin{outR}
Rows: 343 Columns: 11                                             
â”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Delimiter: ","
chr  (1): sex
dbl (10): predynastic, c4800BC, c4200BC, c4000BC, c3700BC, c35...

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{outR}

Running the above code presents us with some useful information about the data set we have loaded.  We can see that it has 343 rows and 11 columns, uses a \R{,} as a delimiter. One column, \R{sex}, consists of character (\R{chr}) values and the remaining 10 columns consist of \R{dbl} values, which is a shorthand way of referring to \textit{double-precision number}. To simplify a complex story, R has multiple types of \textit{numeric} objects; i.e., it has multiple ways of representing a number. A \textit{double}, as its often referred to, is one such representation. If that is confusing, don't worry, what is important to take away from the output is that \R{dbl} means the column contains numeric values (i.e., we can use them to do mathematics).

Running \R{skulls} will print the data frame to the console.

\begin{inR}
skulls
\end{inR}

\begin{outR}
# A tibble: 343 Ã— 11
   sex   predynastic c4800BC c4200BC c4000BC c3700BC c3500BC c2780BC
   <chr>       <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>
 1 Male         1370    1410    1320    1445      NA    1395    1425
 2 Male         1250    1445    1565    1540      NA    1420    1505
 3 Male         1430    1440    1600    1565      NA    1380    1360
 4 Male         1350    1340    1460    1710      NA    1260    1385
 5 Male         1130    1460    1520    1690      NA    1285    1350
 6 Male         1670    1290    1440    1775      NA    1505    1440
 7 Male         1195    1290    1740    1390      NA    1230    1400
 8 Male         1500    1385    1410    1620      NA    1250    1255
 9 Male         1325    1290    1510    1500      NA    1315    1450
10 Male         1480    1565    1550    1255      NA    1360    1310
# i 333 more rows
# i 3 more variables: c1590BC <dbl>, c378BC <dbl>, c331BC <dbl>
# i Use `print(n = ...)` to see more rows
\end{outR}

You can now subset and manipulate the data frame \R{skulls} like was done in Chapter 1 when we first discussing data frames (see section \ref{sec:data_frames}). For instance, if we wanted to look at the mean breadth of skulls from the predynastic period, we could simply run:

\begin{inR}
mean(skulls$predynastic, na.rm = TRUE)
\end{inR}
\begin{outR}
[1] 1320.142
\end{outR}

\noindent
One thing that is not apparent from the 10 row output is that \textit{all} of the numeric columns contain some missing, \R{NA}, values. This is why the \R{na.rm = TRUE} needed to be specified.

\subsubsection{\texttt{read.csv()} vs. \texttt{read\_csv()}}

To load the \R{skulls} data frame above, we used the function \R{read\_csv()}, which is part of the \textit{tidyverse}. However, base R has a similar function, \R{read.csv()}, that will do essentially the same thing - it will read a \texttt{.csv} file in to R. For most use cases there is little advantage to adopting one function over the other, but if you have the \textit{tidyverse} loaded, you may as well use \R{read\_csv()} because it does have some advantages over its predecessor. First, it offers excellent customization options, which are particularly useful when loading very large datasets or merging multiple datasets. Second, it alerts you to any issues encountered during the loading process. Third, it performs much faster under heavy loads than its base R counterpart, even providing a progress bar when reasonable to do so. Finally, instead of a data frame, it stores the data as a \textit{tibble}, which will be discussed later.

\subsection{Reading Other File Types into R}

If your data is delimited by some character other than a comma (e.g., a semicolon, tab, backslash, etc.), there is a more general function that can be employed called \R{read\_delim()} which allows you to specify any delimiter (i.e., separator) using the argument \R{delim}. For instance, we could have loaded the \R{skulls} data in the following way:

\begin{inR}
skulls <- read_delim("Max_Breadth_TRM_1905.csv", delim = ",")
\end{inR}

\vspace{1em}

\noindent
If your text document was separated by semicolons you would just include \R{delim = ";"}, if it was separated using tabs you would just \R{delim = "\textbackslash t"}, and so on. 

One thing that is worth appreciating about delimited files is that their file extension (e.g., the \texttt{.csv} or \texttt{.tsv} at the end of the file name) is irrelevant to how R reads the file. As has been previously emphasized, \texttt{.csv} files and \texttt{.tsv} files for instance, are just generic text documents, nothing more. This means you may see them with the file extension \texttt{.txt}, but that will not impact how any of the above functions operate.

Now, what would you do if you wanted to load a Microsoft Excel spreadsheet file (i.e., a \texttt{.xlsx} file) into R directly? Well as per the discussion on spreadsheets and ethical file formats (see section \ref{sec:spreadsheet_soft} and \ref{sec:ethical_file}), the best practice is to save it as a \texttt{.csv} using Excel and load that new file directly into R. However, should you wish to eschew this advice, the \textit{tidyverse} does have a package called \textit{readxl} with functions that will allow you to do this. This is not part of the nine core packages, so it will need to be loaded using the \R{library()} function. A word of warning is in order though. As well made as the \textit{readxl} package is, reading \texttt{.xlsx} files directly will, almost certainly, cause more problems than it solves. These files are not intended to be read by anything other than Excel and Microsoft does not want them read by anything other than Excel. Thus, by loading the \texttt{.xlsx} file directly into R, you are (computationally speaking) picking an unnecessary fight with Microsoft. Nine times out of ten, you will win that fight thanks to \textit{readxl}, but you will still probably end up with some nasty bruises and scars.

\section{Tibbles vs. Data Frames}

In the output for \R{skulls} (and the \R{msleep} data from chapter 2) you can see that the output printed to the console specifies that we are looking at something called a \gls{tibble}. The output also helpfully displays the dataset's dimensions and the class of object contained within each column. This is in contrast to the data frame created in chapter 1, which did not do any of that for us. In the \textit{tidyverse's} own words, a tibble is a ...

\begin{displayquote}
\headingfont
%\large
modern reimagining of the data.frame, keeping what time has proven to be effective, and throwing out what is not. Tibbles are data.frames that are lazy and surly: they do less (i.e. they donâ€™t change variable names or types, and donâ€™t do partial matching) and complain more (e.g. when a variable does not exist). This forces you to confront problems earlier, typically leading to cleaner, more expressive code. Tibbles also have an enhanced print( ) method which makes them easier to use with large datasets containing complex objects.

- \textit{\url{https://tibble.tidyverse.org/} (2024/07/28)}

\end{displayquote}

In terms of basic usage, tibbles function almost identically to the classic data frame discussed in chapter 1. For instance, with the \textit{tidyverse} loaded, we can re-create chapter 1's data frame as a tibble using an identical syntax.

\begin{inR}
df <- tibble(
  Subject = 1:10,
  Group = c("Exp", "Cont", "Exp", "Cont", "Exp", "Exp",
            "Cont", "Exp", "Cont", "Cont"),
  Value = c(-0.36,  0.28,  1.54,  0.51, -1.28,  1.15,
            -2.22, -0.51,  NA, -1.04)
)

df
\end{inR}

\begin{outR}
# A tibble: 10 Ã— 3
   Subject Group Value
     <int> <chr> <dbl>
 1       1 Exp   -0.36
 2       2 Cont   0.28
 3       3 Exp    1.54
 4       4 Cont   0.51
 5       5 Exp   -1.28
 6       6 Exp    1.15
 7       7 Cont  -2.22
 8       8 Exp   -0.51
 9       9 Cont  NA   
10      10 Cont  -1.04
\end{outR}

\noindent
There are a number of interesting differences between tibbles and data frames, but nothing that merits any in depth discussion for a beginner with R. For the most part they behave identically. However, there is one difference worth mentioning: for tibbles, indexing a single column by specifying row and column values outputs a tibble. For example, suppose we use our index brackets, \R{[ ]}, to isolate the first 5 rows of column 3 in the \R{skulls} data.

\begin{inR}
skulls[1:5, 3]
\end{inR}

\begin{outR}
# A tibble: 5 Ã— 1
  c4800BC
    <dbl>
1    1410
2    1445
3    1440
4    1340
5    1460
\end{outR}

\noindent This seems sensible enough behaviour, but is in contrast to the traditional behaviour of R's data frame which will output a vector unless more than one column is selected.

\begin{inR}
# Using read.csv to load the data as a data frame
skulls_df <- read.csv("skull_cap_partial_wide.csv")

skulls_df[1:5, 3]
\end{inR}

\begin{outR}
[1] 1410 1445 1440 1340 1460
\end{outR}

This may seem to be a trivial distinction; however, operations such as computing the mean of a column are quite common and often require inserting a numeric vector. Consequently, when the output is a tibble rather than a numeric or logical vector, attempting such operations results in an error.

\begin{inR}
mean(skulls[1:5, 3])
\end{inR}

\begin{outR}
[1] NA
Warning message:
In mean.default(skulls[1:5, 3]) :
  argument is not numeric or logical: returning NA
\end{outR}

\noindent However, you can set the argument \R{drop = TRUE} inside the indexing brackets to coerce the output into a vector.

\begin{inR}
skulls[1:5, 3, drop = TRUE]
mean(skulls[1:5, 3, drop = TRUE])
\end{inR}

\begin{outR}
[1] 1410 1445 1440 1340 1460
[1] 1419
\end{outR}

Should the need arise, switching between tibbles and data frames is a simple matter. For example, to convert our tibble \R{skulls} to a data frame, we can simply use the \R{as.data.frame()} function in R.

\begin{inR}
# tibble to data frame
skulls <- as.data.frame(skulls)
skulls
\end{inR}
\begin{outR}
   sex  predynastic c4800BC c4200BC c4000BC c3700BC c3500BC c2780BC
1  Male        1370    1410    1320    1445      NA    1395    1425
2  Male        1250    1445    1565    1540      NA    1420    1505
3  Male        1430    1440    1600    1565      NA    1380    1360
4  Male        1350    1340    1460    1710      NA    1260    1385
5  Male        1130    1460    1520    1690      NA    1285    1350
6  Male        1670    1290    1440    1775      NA    1505    1440
7  Male        1195    1290    1740    1390      NA    1230    1400
8  Male        1500    1385    1410    1620      NA    1250    1255
9  Male        1325    1290    1510    1500      NA    1315    1450
10 Male        1480    1565    1550    1255      NA    1360    1310
...
\end{outR}

\noindent
To convert it back to a tibble:
\begin{inR}
# data frame to tibble
skulls <- as_tibble(skulls)
skulls
\end{inR}
\begin{outR}
# A tibble: 343 Ã— 11
   sex   predynastic c4800BC c4200BC c4000BC c3700BC c3500BC c2780BC
   <chr>       <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>
 1 Male         1370    1410    1320    1445      NA    1395    1425
 2 Male         1250    1445    1565    1540      NA    1420    1505
 3 Male         1430    1440    1600    1565      NA    1380    1360
 4 Male         1350    1340    1460    1710      NA    1260    1385
 5 Male         1130    1460    1520    1690      NA    1285    1350
 6 Male         1670    1290    1440    1775      NA    1505    1440
 7 Male         1195    1290    1740    1390      NA    1230    1400
 8 Male         1500    1385    1410    1620      NA    1250    1255
 9 Male         1325    1290    1510    1500      NA    1315    1450
10 Male         1480    1565    1550    1255      NA    1360    1310
# i 333 more rows
# i 3 more variables: c1590BC <dbl>, c378BC <dbl>, c331BC <dbl>
# i Use `print(n = ...)` to see more rows
\end{outR}

\subsection{Displaying Tibbles in the Console}

\subsubsection{Tibble Dimensions}

Given the limited screen space and the large size of most datasets, tibbles are designed to display only the first 10 rows when printed to the console, making it easier for users to work with their data.

Generally, if you want to view an entire data set, the best practice is not to display it in the console but rather use R's \R{View()} function which opens it in a spreadsheet-style window. That being said, many people will still find the number of rows displayed by a tibble within the console lacking, particularly if you are working on anything other than a small laptop. For this reason, the 10 row limit is a behaviour which can be circumvented in various ways. One simple way is to make use of the \R{print()} function. For instance, if we want to display the first 20 rows we can simply run ...

\begin{inR}
print(skulls, n = 20)
\end{inR}

\vspace{1em}

An alternative method is to change R's default display behaviour by setting the minimum number of rows to output using the \R{options()} function.

\begin{inR}
options(pillar.print_min = 20)
skulls
\end{inR}

\vspace{1em}

\noindent
If that method is your preference, then it is usually advisable to place the \R{options()} code at the top of your R script because it only needs to be run once.\footnote{If you are wondering why we specify \R{pillar...} to set rows, it's because \textit{pillar} is a package in the \textit{tidyverse}.}

What if you wanted to display every single row each time you print a tibble? Well, recall that R represents infinity in the positive direction as (\R{Inf}). We can use that to our advantage here:

\begin{inR}
options(pillar.print_min = Inf)
skulls
\end{inR}

\vspace{1em}

What about columns though? Well, interestingly tibbles will actually conform to the size of your console screen.  So if you can only fit five columns on screen, the tibble will only display those five and notify you of the others not displayed beneath the output. This is done to preserve the ``rectangleness'' of the data so it can be visualized appropriately. This also stands in stark contrast to how base R's data frames behave, which will stack columns on top of each other, seemingly with no consideration of column or row space. Admittedly, its nice to have all that information displayed, but it comes at the cost of being difficult for a human to visually parse. That being said, if you wanted your tibbles to behave like this and always display all columns, you can just add an additional argument, \R{pillar.width = Inf}, to the \R{options()} function:

\begin{inR}
options(
  pillar.print_min = 20,
  pillar.width = Inf
)

skulls
\end{inR}

\vspace{1em}

\noindent
However, if you prefer a more temporary solution, you can just add a \R{width} argument to the \R{print()} function. E.g., 

\begin{inR}
print(skulls, n = 20, width = Inf)
\end{inR}

\vspace{1em}

\subsubsection{The Precision and Display of Decimals in Tibbles}
\label{sec:sig_digs}

To save space and facilitate easier reading, both tibbles and data frames will round values with many decimal values. Though, in the case of tibbles, they do not just simply round to a preset number of digits.  To illustrate what tibbles are doing in this respect, recall that R has a built-in constant for $\pi$.

\begin{inR}
pi
\end{inR}
\begin{outR}
[1] 3.141593
\end{outR}

\noindent
Using that, we will create a simple tibble that repeats $\pi$ four times within a single column.

\begin{inR}
pi_df <- tibble(pie = rep(pi, 4))
pi_df
\end{inR}
\begin{outR}
# A tibble: 10 Ã— 1
     pie
   <dbl>
 1  3.14
 2  3.14
 3  3.14
 4  3.14
\end{outR}

\noindent
One thing that will be noticed is that the tibble is only displaying $\pi$ to two decimal places. However, all of the digits still exist in R's memory and any calculations you do will take those unseen digits into account. For instance, if we isolate the first row's value you can see that all the digits of $\pi$ are displayed.

\begin{inR}
print(pi_df$pie[1], digits = 16)
\end{inR}
\begin{outR}
[1] 3.141592653589793
\end{outR}

\noindent
It is important to understand that tibbles do not strictly control the handling of decimals. Instead, they work with \gls{significant figures}, also known as \textit{significant digits}.  This allows the tibble to preserve its desired ``rectangleness,'' giving it cleaner looking columns. Since tibbles default to three significant figures, $\pi$ will only display as 3.14. 

As a refresher of primary school math, with significant figures, everything in front of the decimal point is always displayed, but each number in front of that decimal point uses up a significant figure (a.k.a. a ``sig fig'' or ``sig dig''). For instance, if you had a number like 666.13.  Displaying that to two sig figs would give you 666.  Displayed to three sig figs would again be 666. Displayed to four sig figs would be 666.1. Five sig digs would be 666.13. Six sig figs would be 666.130. Seven would be 666.1300, and so on.

To increase the number of sig figs shown within a tibble we can simply add another argument to the \R{options()} function:

\begin{inR}
options(pillar.sigfig = 16)
pi_df
\end{inR}
\begin{outR}
# A tibble: 10 Ã— 1
                   pie
                 <dbl>
 1 3.141592653589793e0
 2 3.141592653589793e0
 3 3.141592653589793e0
 4 3.141592653589793e0
\end{outR}

Because of limitations of 64-bit computing, a tibble is not going let you exceed 16 sig figs and in certain cases will display results in scientific notation. In this case we see some scientific notation, but it is to the power of 0, so it can be ignored.

\section{Wide Data vs. Tidy Data}

\subsection{Wide Data}
\label{sec:wide_data}

Examining the \R{skull\_cap\_partial\_wide.csv} file loaded at the beginning of this chapter, we see that the data is structured logically. The first column represents the presumed sex (male or female) of the skulls in each row, as recorded by \textcite{Thomson1905}. The remaining columns represent skull measurements from a specific period in Egypt's history. Note that the output below displays only the first 8 of 11 columns, and the number of columns visible in your output may vary depending on your screen size.

\begin{inR}
skulls
\end{inR}

\begin{outR}
# A tibble: 343 Ã— 11
   sex   predynastic c4800BC c4200BC c4000BC c3700BC c3500BC c2780BC
   <chr>       <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>
 1 Male         1370    1410    1320    1445      NA    1395    1425
 2 Male         1250    1445    1565    1540      NA    1420    1505
 3 Male         1430    1440    1600    1565      NA    1380    1360
 4 Male         1350    1340    1460    1710      NA    1260    1385
 5 Male         1130    1460    1520    1690      NA    1285    1350
 6 Male         1670    1290    1440    1775      NA    1505    1440
 7 Male         1195    1290    1740    1390      NA    1230    1400
 8 Male         1500    1385    1410    1620      NA    1250    1255
 9 Male         1325    1290    1510    1500      NA    1315    1450
10 Male         1480    1565    1550    1255      NA    1360    1310
# i 333 more rows
# i 3 more variables: c1590BC <dbl>, c378BC <dbl>, c331BC <dbl>
# i Use `print(n = ...)` to see more rows
\end{outR}

This style of layout makes it easy to do certain things with the data. For instance, if we wanted to know the mean breadth of skulls \textit{circa} 4800 BCE, we could just run 

\begin{inR}
mean(skulls$c4800BC, na.rm = TRUE)
\end{inR}

\begin{outR}
[1] 1348.831
\end{outR}

If we wanted to calculate the mean of each column, we can use the \R{apply()} function. This function literally \textit{applies} a function of your choosing to either the columns or rows. For example, we might use \R{apply()} to compute the mean of each column. However, the first column (\R{\$sex}) contains character (\R{chr}) values which we cannot take the mean of. Trying to do so will generate an error, so we need to exclude such columns before performing our calculations. We can use what we learned about indexing in chapter 1 to ignore that first column (see section \ref{sec:df_Index}).

\begin{inR}
apply(skulls[, 2:11], MARGIN = 2, FUN = mean, na.rm = TRUE)
\end{inR}

\begin{outR}
predynastic     c4800BC     c4200BC     c4000BC     c3700BC     c3500BC 
   1320.142    1348.831    1434.375    1494.700    1356.429    1336.663 
   
    c2780BC     c1590BC      c378BC      c331BC 
   1308.454    1347.488    1285.781    1318.642 
\end{outR}

\noindent
The argument \R{FUN} specifies what function is applied. The argument \R{MARGIN} specifies whether that function is applied to the rows (\R{1}) or columns (\R{2}). In this case we are applying it to columns, so we specified \R{MARGIN = 2}. \R{na.rm} is of course an argument belonging to the \R{mean()} function, which we need to specify so R knows how to handle the \R{NA} values in the data.

As another example of \R{apply()}, if you wanted to know the maximum value contained in each row, ignoring the first column, you could \textit{apply} the function \R{max()} to the \textit{rows}. Since our data has 343 rows, we will produce a vector of 343 elements, of which only the first 130 are shown below.

\begin{inR}
apply(skulls[, 2:11], MARGIN = 1, FUN = max, na.rm = TRUE)
\end{inR}

\begin{outR}
  [1] 1450 1565 1600 1710 1690 1775 1740 1620 1585 1565 1600 1560 1590
 [14] 1575 1630 1670 1570 1510 1525 1475 1610 1460 1475 1570 1485 1560
 [27] 1570 1570 1510 1540 1495 1520 1640 1600 1515 1590 1450 1740 1515
 [40] 1520 1525 1510 1490 1475 1560 1505 1630 1535 1530 1600 1510 1550
 [53] 1560 1590 1480 1570 1615 1510 1610 1430 1585 1610 1660 1470 1570
 [66] 1665 1440 1520 1560 1460 1485 1385 1530 1485 1470 1510 1430 1420
 [79] 1610 1610 1560 1580 1500 1465 1440 1595 1425 1485 1575 1595 1550
 [92] 1720 1535 1370 1400 1345 1630 1340 1400 1420 1455 1380 1530 1480
[105] 1360 1500 1615 1465 1400 1500 1475 1560 1550 1400 1610 1360 1630
[118] 1760 1325 1400 1510 1640 1635 1620 1435 1305 1490 1545 1465 1540
...
\end{outR}

The structure of this dataset is what is commonly referred to as the \gls{wide format}. At first glance, R makes it fairly easy to work with data in this format. However, we have not attempted anything particularly complex yet and, apart from the first column, all the columns are conveniently numeric. Even though this layout might seem intuitive, it is not ideal for many types of analysis or visualization. In fact, the original dataset\footnote{\url{https://github.com/statistical-grimoire/book/blob/main/data/Egyptian-skulls/Thomson_Randall-MacIver_1905.csv}} had to be substantially truncated to fit into a usable wide format for our purposes here.

\subsection{Tidy data}

Despite its shortcomings, use of the \textit{wide format} is fairly common and gets its name from the fact that it spreads variables across multiple columns. In this case, we can treat of the different historical periods of the skulls (predynastic, c4800BC, c4200BC, c4000BC, etc.) as a single variable in its own right. We might call this variable simply ``\textit{period}.'' That is to say, c4800BC represents a \textit{period} in Egypt's history, c4200BC represents a \textit{period} in Egypt's history, c4000BC represents a \textit{period} in Egypt's history, and so on. Adjunct to this is a second variable, which we could refer to as the ``\textit{cranial capacity}'' of the skulls. This variable consists of the literal measurements 1370, 1250, 1430, and so on. And of course ``\textit{sex}'' is a variable in this data as well; however, it is not spread across multiple columns the way \textit{period} and \textit{cranial capacity} are. 

When organizing or arranging data, best practices dictate that you \textbf{restrict a single variable to a single column}. In this case, the variable \textit{period} is being spread across ten columns. And the variable \textit{cranial capacity} is found within each of those ten columns. To fix this, we can use the \textit{tidyverse} function \R{pivot\_longer()}.

\begin{inR}
skulls_tidy <- pivot_longer(skulls,
  cols = predynastic : c331BC,
  names_to = "period",
  values_to = "capacity"
)
skulls_tidy
\end{inR}
\begin{outR}
# A tibble: 3,430 Ã— 3
   sex   period      capacity
   <chr> <chr>          <dbl>
 1 Male  predynastic     1370
 2 Male  c4800BC         1410
 3 Male  c4200BC         1320
 4 Male  c4000BC         1445
 5 Male  c3700BC           NA
 6 Male  c3500BC         1395
 7 Male  c2780BC         1425
 8 Male  c1590BC         1440
 9 Male  c378BC          1310
10 Male  c331BC          1450
# i 3,420 more rows
# i Use `print(n = ...)` to see more rows
\end{outR}

Notice a few differences with this pivoted data. There are now considerably more rows in the data, 3,430 vs. 343, and there are two new columns, \R{\$period} and \R{\$capacity} which have replaced the ten different time period columns. In terms of the \R{pivot\_longer()} function we used, the most important argument we specified is \R{cols}, as this defines which columns are going to be collapsed into a single column. The arguments \R{names\_to} and \R{values\_to} just specify the name of the new columns and are not strictly required, but are good practice to include.

In the above code, we used \R{:} to specify a range of columns (from predynastic to c331BC), but we could have instead provided a vector of column names, as shown below:

\begin{inR}
cols <- c(
  predynastic, c4800BC, c4200BC, c4000BC, c3700BC,
  c3500BC, c2780BC, c1590BC, c378BC, c331BC
)
\end{inR}
\vspace{1em}

\noindent It is worth noting that the \textit{tidyverse} has numerous methods for selecting multiple columns simultaneously that do not exist as part of base R. For a rundown of each see the R documentation: \R{?tidyr\_tidy\_select}. Additionally, if you are operating outside of the \textit{tidyverse}, analogous base R functions will usually require column names to be entered as character strings; though, \textit{tidyverse} functions are typically indifferent to this practice. E.g.,

\begin{inR}
cols <- c(
  "predynastic", "c4800BC", "c4200BC", "c4000BC", "c3700BC",
  "c3500BC", "c2780BC", "c1590BC", "c378BC", "c331BC"
)
\end{inR}
\vspace{1em}

By collapsing the 10 period columns into one, the data has now ascended into a sacred arrangement known as \gls{tidy data} (or, as some heretics call it, ``the long format'').  Tidy data is a cornerstone of the \textit{tidyverse's} thaumaturgy and all tidy data adheres to three basic precepts:

\begin{minipage}{\textwidth}

\begin{enumerate}[label=\Roman*.]
\IMFellEnglish
    %\setlength\itemsep{-1em}
    %\large
    \item Each variable is a column; each column is a variable.
    \item Each observation is a row; each row is an observation.
    \item Each value is a cell; each cell is a single value.
\end{enumerate}
\end{minipage}

\noindent
It can be seen that the skull data now satisfies these three standards, as did the \R{msleep} data used in chapter 2. As we progress through the remainder of this chapter, it will become apparent that having your data in this tidy form will greatly facilitate both plotting and analysis. 

Interestingly, because of the restriction that data frames and tibbles have whereby each column needs to contain the same amount of elements, the wide data necessarily had to make use of \R{NA} values. The pivoting that was done to transform the data from wide to tidy preserved those \R{NA} values, but now there is no need for them to keep them in the data (because they just represent a non-existent value). As an example, consider row five:

\begin{inR}
skulls_tidy[5, ]
\end{inR}
\begin{outR}
# A tibble: 1 Ã— 3
  sex   period  capacity
  <chr> <chr>      <dbl>
1 Male  c3700BC       NA
\end{outR}

\noindent
Without a value for \R{\$capacity}, this row conveys no meaningful information. As such, itâ€”and any rows like itâ€”can be safely removed from the dataset.

\begin{inR}
skulls_tidy <- drop_na(skulls_tidy, capacity)
skulls_tidy
\end{inR}
\begin{outR}
# A tibble: 1,449 Ã— 3
   sex   period      capacity
   <chr> <chr>          <dbl>
 1 Male  predynastic     1370
 2 Male  c4800BC         1410
 3 Male  c4200BC         1320
 4 Male  c4000BC         1445
 5 Male  c3500BC         1395
 6 Male  c2780BC         1425
 7 Male  c1590BC         1440
 8 Male  c378BC          1310
 9 Male  c331BC          1450
10 Male  predynastic     1250
# i 1,439 more rows
# i Use `print(n = ...)` to see more rows
\end{outR}

\noindent
The function \R{drop\_na()} simply removes (i.e., ``drops'') any rows that have a \R{NA} value in the columns you specify.

\section{Laying Pipe (The \texttt{|>} and \texttt{\%>\%} Operators)}

One of the most significant contributions of the \textit{tidyverse} to R has been its seamless implementation of what is known as `piping' syntax, which allows for an almost otherworldly level of efficiency. However, this is not to say that piping was a concept invented entirely by the \textit{tidyverse}. Rather, the \textit{tidyverse}'s consistent and innovative use of it brought its potential to light, leading to widespread adoption within the R community. The best evidence of this influence is the integration of piping into base R as of \href{https://stat.ethz.ch/pipermail/r-announce/2021/000670.html}{version 4.1.0}, released in 2021.\footnote{Although I have no direct evidence that the \textit{tidyverse} directly motivated the addition of the pipe operator to base R, it seems unlikely to be a coincidence given the ubiquity of \R{\%>\%} and the popularity of the \textit{dplyr} package.} But what exactly is `piping'?

The essence of piping is that you are transferring the output of one thing to another. For instance, suppose we wanted to know the mean cranial capacity across all periods of skulls. We could of course insert the \R{\$capacity} column into the mean function like so ...

\begin{inR}
mean(skulls_tidy$capacity)
\end{inR}
\begin{outR}
[1] 1335.255
\end{outR}

Alternatively, we could ``pipe'' (i.e., transfer) the \R{\$capacity} column in our tidy data to the \R{mean()} function using R's pipe operator \R{|>}

\begin{inR}
skulls_tidy$capacity |> mean()
\end{inR}
\begin{outR}
[1] 1335.255
\end{outR}

\noindent We could then send that output to something like the \R{round()} function.

\begin{inR}
skulls_tidy$capacity |>
  mean() |>
  round(1)
\end{inR}
\begin{outR}
[1] 1335.3
\end{outR}

As another example, suppose we wanted a tidy data frame that only contained skulls from the predynastic period. The standard methodology would be to specify the data frame within the \R{filter()} function, like so ...

\begin{inR}
filter(skulls_tidy, period == "predynastic")
\end{inR}
\begin{outR}
# A tibble: 318 Ã— 3
   sex   period      capacity
   <chr> <chr>          <dbl>
 1 Male  predynastic     1370
 2 Male  predynastic     1250
 3 Male  predynastic     1430
 4 Male  predynastic     1350
 5 Male  predynastic     1130
 6 Male  predynastic     1670
 7 Male  predynastic     1195
 8 Male  predynastic     1500
 9 Male  predynastic     1325
10 Male  predynastic     1480
# i 308 more rows
# i Use `print(n = ...)` to see more rows
\end{outR}

\noindent
Alternatively, we could pipe the data into the \R{filter()} function to achieve the exact same result.

\begin{inR}
skulls_tidy |> filter(period == "predynastic")
\end{inR}

\vspace{1em}

In addition to this, suppose you did not want the \R{\$sex} column inside the output. To achieve this, this output could be further piped into the \textit{tidyverse's} \R{select()} function which allows you to grab specific columns.\footnote{Note that we could obtain the same result by excluding the sex column in the function. i.e., \R{select(-c(sex))}}

\begin{inR}
skulls_tidy |> 
  filter(period == "predynastic") |> 
  select(period, capacity)
\end{inR}
\begin{outR}
# A tibble: 318 Ã— 2
   period      capacity
   <chr>          <dbl>
 1 predynastic     1370
 2 predynastic     1250
 3 predynastic     1430
 4 predynastic     1350
 5 predynastic     1130
 6 predynastic     1670
 7 predynastic     1195
 8 predynastic     1500
 9 predynastic     1325
10 predynastic     1480
# i 308 more rows
# i Use `print(n = ...)` to see more rows
\end{outR}

Now that the logic of piping is clear, it is worth reiterating that the \R{|>} operator is a relatively new arrival in base R. Prior to its introduction in R \href{https://stat.ethz.ch/pipermail/r-announce/2021/000670.html}{version 4.1.0}, the convention would be to use the \textit{tidyverse's} pipe operator \R{\%>\%} instead. This comes from a package called \textit{magrittr} which contains a variety of pipes for different purposes, but the most significant of these is \R{\%>\%}. This was, and to a certain extent still is, the de facto pipe used by the R community at large. However, the recommended wisdom now (by the keepers of the \textit{tidyverse}) is to use base R's pipe and not \textit{magrittr's}. That being said, many are unaware of this update to base R and much of the help documentation on websites like \href{https://stackoverflow.com/}{stack overflow} still use \textit{magrittr's} \R{\%>\%}. In terms of functionality, there is little meaningful difference between \R{|>} and \R{\%>\%} and all of the above code could have been written using \R{\%>\%}.

The above examples nicely show how the pipe operator works, but we should consider a more realistic use case to illustrate its versatility. 

\subsection{Data Manipulation Example}

\subsubsection{Summarising the Data}

The \R{skull\_cap\_partial\_wide.csv} data we loaded earlier was of course in the wide format originally, which is rarely needed. So what we could have done instead is loaded that data in to R using \R{read\_csv}, pipe it to the \R{pivot\_longer()} function, and then pipe that into the \R{drop\_na()} function.

\begin{inR}
skulls <- read_csv("skull_cap_partial_wide.csv") |>
  pivot_longer(
    cols = predynastic:c331BC,
    names_to = "period",
    values_to = "capacity"
  ) |>
  drop_na(capacity)

skulls
\end{inR}
\begin{outR}
# A tibble: 1,449 Ã— 3
   sex   period      capacity
   <chr> <chr>          <dbl>
 1 Male  predynastic     1370
 2 Male  c4800BC         1410
 3 Male  c4200BC         1320
 4 Male  c4000BC         1445
 5 Male  c3500BC         1395
 6 Male  c2780BC         1425
 7 Male  c1590BC         1440
 8 Male  c378BC          1310
 9 Male  c331BC          1450
10 Male  predynastic     1250
# i 1,439 more rows
# i Use `print(n = ...)` to see more rows
\end{outR}

It is worth emphasizing the utility of the pipe operator here: It allowed us to get our data into the form we wanted without creating and calling multiple different objects in memory. Only one object was created, \R{skulls}. Moreover, the ``arrow-like'' notation of the pipe \R{|>} nicely shows the workflow, i.e., logic, of our code.

Now suppose we wanted to compute some summary statistics for this data set.  For instance, maybe we want to know the mean cranial capacity of each period. This is where the \textit{tidyverse's} functions \R{group\_by()} and \R{summarise()} become extremely useful. Both of these functions, as well as the \R{filter()} and \R{select()} functions we have been using, come from a very influential \textit{tidyverse} package called \textit{dplyr}. 

\begin{mdframed}[nobreak = true, style = miscFrame, frametitle = \Large\IMFellEnglish Box 3.1: Why is it called \textit{dplyr}?]
\color{darkgray}
\IMFellEnglish

Generally, the names of R packages are relatively intuitive or are based on an initialism of some kind. The \textit{dplyr} package is an exception to that. The package's strange name is a reference to both pliers (the tool) and a family of functions based around the \R{apply()} function that we briefly used in section \ref{sec:wide_data}. The \enquote{d} refers to data frames. i.e., it is as if you are taking a pair of pliers to data frames.

A common go-to strategy of programmers generally is to use for-loops to do much of the computational grunt work. For-loops just repeatedly execute a set of code until some condition has been satisfied. While for-loops can be used in R, its users often prefer to take a different, more efficient, \enquote{vectorized} approach. The goal is to use what are called \glspl{functional}. These are functions that accept another function as an input and produce a vector as output. That is precisely what the \R{apply()} function and its relatives like \R{lapply}, \R{sapply}, \R{vapply} do. R is incredibly adept at working with vectors, matrices, and arrays, and \textit{dplyr's} functions are all based around a strategy of using functionals for data manipulation.
\end{mdframed}

We will begin with the \R{summarise()} function which is used to create a data frame based on columns/variables in your data.

\begin{inR}
skulls |> 
  summarise(m = mean(capacity))
\end{inR}

\begin{outR}
# A tibble: 1 Ã— 1
      m
  <dbl>
1 1335.
\end{outR}

The code we have written is telling the \R{summarise()} function to apply the \R{mean()} function to the \R{\$capacity} column. When it did this, it also created a new data frame\footnote{Yes, yesâ€”I know, \textit{technically} what we created was a \textit{tibble}. But thatâ€™s only because \R{skulls} was already a tibble to begin with. For the sake of sanity (mine and yours), Iâ€™ll be treating tibbles and data frames as interchangeable for the rest of this book. Purists, feel free to clutch your pearls.} and stored that calculation as a column called \R{\$m} (though, we could have named the column whatever we wanted).

At present, none of this may seem terribly useful; however, we can make it more useful by including the \R{group\_by()} function which will tell R to literally ``group by'' categories found in a different column or set of columns. Specifically, we can tell it to group by \R{\$period} and then summarise the data.

\begin{inR}
skulls |> 
  group_by(period) |> 
  summarise(m = mean(capacity))
\end{inR}
\begin{outR}
# A tibble: 10 Ã— 2
   period          m
   <chr>       <dbl>
 1 c1590BC     1347.
 2 c2780BC     1308.
 3 c331BC      1319.
 4 c3500BC     1337.
 5 c3700BC     1356.
 6 c378BC      1286.
 7 c4000BC     1495.
 8 c4200BC     1434.
 9 c4800BC     1349.
10 predynastic 1320.
\end{outR}

We can now see the mean of each period in the data set and if we wanted, we could create another column, \R{\$n}, showing how many skulls there are in each period total by using the \R{length()} function to count the skulls.

\begin{inR}
skulls |>
  group_by(period) |>
  summarise(
    m = mean(capacity),
    n = length(capacity)
  )
\end{inR}
\begin{outR}
# A tibble: 10 Ã— 3
   period          m     n
   <chr>       <dbl> <int>
 1 c1590BC     1347.   203
 2 c2780BC     1308.   152
 3 c331BC      1319.   232
 4 c3500BC     1337.   315
 5 c3700BC     1356.     7
 6 c378BC      1286.    32
 7 c4000BC     1495.    50
 8 c4200BC     1434.    16
 9 c4800BC     1349.   124
10 predynastic 1320.   318
\end{outR}

If we wanted to add in a column, \R{\$N}, that represented the total amount of skulls across all the periods we could count the number of rows in \R{\$skulls}...

\begin{inR}
nrow(skulls)
\end{inR}
\begin{outR}
[1] 150
\end{outR}

\noindent
... and include that in the \R{summarise()} function.

\begin{inR}
skulls |> 
  group_by(period) |> 
  summarise(
    m = mean(max_breadth),
    n = length(max_breadth),
    N = nrow(skulls)
    )
\end{inR}
\begin{outR}
# A tibble: 5 Ã— 4
  period        m     n     N
  <chr>     <dbl> <int> <int>
1 c150_CE    136.    30   150
2 c1850_BCE  134.    30   150
3 c200_BCE   136.    30   150
4 c3300_BCE  132.    30   150
5 c4000_BCE  131.    30   150
\end{outR}

Mathematical expressions can also be applied to columns created within the \R{summarise()} function. For example, the mean maximum breadth is calculated in millimetres, but it can be easily converted to another unit, such as centimetres, by referencing the column and performing a simple mathematical operation.

\begin{inR}
skulls |> 
  group_by(period) |> 
  summarise(
    m = mean(max_breadth),
    n = length(max_breadth),
    N = nrow(skulls),
    m_cm = m / 10
    )
\end{inR}

\begin{outR}
# A tibble: 5 Ã— 5
  period        m     n     N  m_cm
  <chr>     <dbl> <int> <int> <dbl>
1 c150_CE    136.    30   150  13.6
2 c1850_BCE  134.    30   150  13.4
3 c200_BCE   136.    30   150  13.6
4 c3300_BCE  132.    30   150  13.2
5 c4000_BCE  131.    30   150  13.1
\end{outR}


To finish up, lets include the maximum and minimum size found in each period. We will also store this as an tibble called \R{skull\_summary}.

\begin{inR}
skull_summary <- skulls |>
  group_by(period) |>
  summarise(
    m = mean(max_breadth),
    n = length(max_breadth),
    N = nrow(skulls),
    m_cm = m / 10,
    min = min(max_breadth),
    max = max(max_breadth)
  )\textbf{}

skull_summary
\end{inR}

\begin{outR}
# A tibble: 5 Ã— 7
  period        m     n     N  m_cm   min   max
  <chr>     <dbl> <int> <int> <dbl> <dbl> <dbl>
1 c150_CE    136.    30   150  13.6   126   147
2 c1850_BCE  134.    30   150  13.4   126   140
3 c200_BCE   136.    30   150  13.6   129   144
4 c3300_BCE  132.    30   150  13.2   123   148
5 c4000_BCE  131.    30   150  13.1   119   141
\end{outR}

\subsubsection{Plotting the Summarised Data}

Now that we have neatly organized these summary statistics in a tibble, we can visualize them. Since the \R{\$period} column contains five discrete categories, a bar plot is a natural choice for representing these data, so that is what we will create.

The basic logic of plotting has been discussed at length in chapter 2, and this discussion will follow from that.\footnote{In other words, if you haven't read chapter 2, go back and do that.} The first step will be to give \textit{ggplot2} the data and tell it which columns to map to the x and y axis respectively. Then we will add the \R{geom\_bar()} function to this. In this case, we are going to display the mean (i.e., column \R{\$m}) on the y-axis because that is a fairly standard practice many people will be familiar with. Though, it is worth remembering that any of the other numeric columns could be used as well.

\begin{inR}
ggplot(skull_summary, aes(x = period, y = m)) +
  geom_bar(stat = "identity")
\end{inR}

\vspace{2em}

\begin{figure}[H]
\includegraphics[scale = .75]{graphics/ch3Figs/bar_1.pdf}
\end{figure}

The argument \R{stat = "identity"} is simply telling \textit{ggplot2} to use the values within the \R{skull\_summary} tibble to create the bars.  We needed to specify this because \textit{ggplot2} has the ability to take the raw data directly (e.g., \R{skulls}) and perform its own summary calculations. However, we do not need it to do that in this particular case, hence why we included this argument.

The resulting bar graph displays the mean maximum breadth for each period. To enhance its visual appeal, we can adjust the fill colour of the bars to reflect the corresponding time periods more effectively (see Table \ref{tab:egypt_colour}).\footnote{Technically, this is something we should NOT do because, for the sake of comparison, its better to give all the bars the same ``visual weight.'' Keeping all the bars the same colour does precisely that. Moreover, with the x-axis labels, there is no reason to add additional elements that could be distracting. That being said, if you are collaborating on a project, your collaborators will probably demand to see colourful bars irrespective this rationale (experience has taught me this). And if they outnumber you, they can probably beat you in a fight - it doesn't matter if you have the moral or logical high ground.} When we do this, we have to be mindful of the fact that the x-axis contains a \textit{discrete} scale, not a \textit{continuous} one like we saw in chapter 2 (for more information on discrete vs. continuous scales see section \ref{sec:pos_scale}).

\input{tables/ch-3/egypt_colour.tex}

First we will define our colour palette.

\begin{inR}
egypt_pal <- c("#E3C9A8", "#0C2C84", "#D4AF37", "#20603D", "#A23E0E")
\end{inR}

\vspace{1em}

Once \R{egypt\_pal} has been created we can use it to adjust the colour of the bar graph.

\begin{inR}
ggplot(skull_summary, aes(x = period, y = m)) +
  geom_bar(
    stat = "identity",
    colour = "black",
    aes(fill = period)
  ) +
  scale_fill_discrete(type = egypt_pal)
\end{inR}

\vspace{2em}

\begin{figure}[H]
\includegraphics[scale = .75]{graphics/ch3Figs/bar_2.pdf}
\end{figure}

Now, in addition to the mean maximum breadth of each period, \R{skull\_summary} also has information pertaining to the smallest and largest measured skull (these are columns \R{\$min} and \R{\$max} respectively). We could incorporate that information in our graph with the use of \glspl{error bar}. Error bars are a visual representation of our data's \textit{spread}, and the difference between the minimum and maximum represent a classic measure of spread called the \textit{range}.\footnote{If that isn't entirely clear, don't worry. The concept of spread as a statistical term will be explained in more detail in later chapters.}

To create error bars, we can simply use \textit{ggplot2's} \R{geom\_errorbar()} function. We just need to tell it which column corresponds to the bottom of the error bar (\R{ymin}) and which column corresponds to the top of the errorbar (\R{ymax}).

\begin{inR}
ggplot(mm_summary, aes(x = type, y = m)) +
  geom_bar(
    stat = "identity",
    colour = "black",
    aes(fill = type)
  ) +
  scale_fill_discrete(type = mm_palette) +
  geom_errorbar(aes(ymin = min, ymax = max), width = 0.25)
\end{inR}

\vspace{2em}

\begin{figure}[H]
\includegraphics[scale = .75]{graphics/ch3Figs/bar_3.pdf}
\end{figure}

All that remains is to update the plotâ€™s labelling. Specifically, we should provide clearer titles for the $x$- and $y$-axes and remove the underscore (\_) in the $x$-axis labels. The legend can also be removed as it is redundant with the labelling.

To change the current labels ``c150\_CE'', ``c1850\_BCE'', ``c200\_BCE'' and so on, we can use the function \R{scale\_x\_discrete()} and use its \R{labels} argument. We just have to give it a character vector containing the new labelling in the correct order.

To remove the legend, there are different methods you could employ. In this case, since we only have the fill aesthetic mapped, it is easy enough to just add \R{guide = "none"} to the \R{scale\_fill\_discrete()} function.

\begin{inR}
ggplot(skull_summary, aes(x = period, y = m)) +
  geom_bar(
    stat = "identity",
    colour = "black",
    aes(fill = period)
  ) +
  scale_fill_discrete(type = egypt_pal, guide = "none") +
  geom_errorbar(aes(ymin = min, ymax = max), width = 0.25) +
  scale_x_discrete(
    labels = c("c150 CE", "c1850 BCE", "c200 BCE", "c3300 BCE", "c4000 BCE")
  ) +
  xlab("Period") +
  ylab("Maximum Breadth (mm)")
\end{inR}

\vspace{2em}

\begin{figure}[H]
\includegraphics[scale = .75]{graphics/ch3Figs/bar_4.pdf}
\end{figure}

After reading Chapter 2 and this chapter, one key aspect of plotting remains unaddressed: how to adjust the order of categories. Currently, the time periods are not arranged chronologically (left to right) as one might expect. How can we fix this? This is where the concept of factors becomes important.

\section{Factors}

In statistics we often refer to a categorical variable as a \gls{factor},\footnote{Independent, manipulated, and predictor variables are often given this label when they have a fixed set of categories or are continuous but only take on a limited number of discrete values. If this is unclear now, don't worry â€” variable types will be explained in more detail in a later chapter.} and factors have different \glspl{level}.  For instance, in our tidy data (\R{skulls\_tidy}) the variable \R{\$period} could be considered a factor. The individual periods (e.g., c4000 BCE, c3300 BCE, etc.) in that column are each a level of that factor: c4000 BCE is its own level, c3300 BCE is its own level, c1850 BCE is its own level, and so on. In other words, in the \R{skulls\_tidy} data, the factor named ``period'' has 5 levels.

To summarise, you can treat the term ``factor'' as synonymous with the terms ``column'' or ``variable.'' And you can treat the term ``level'' as synonymous with the term ``category.'' Though, this only applies to tidy data, not wide data.
{
\begin{itemize}
  \setlength\itemsep{-1em}
    \item Factor = column / variable
    \item Level = category within a column / variable
\end{itemize}
}

If we examine \R{skulls\_tidy}:

\begin{inR}
mm_data
\end{inR}
\begin{outR}
# A tibble: 150 Ã— 3
   skull period    max_breadth
   <dbl> <chr>           <dbl>
 1     1 c4000_BCE         131
 2     1 c3300_BCE         124
 3     1 c1850_BCE         137
 4     1 c200_BCE          137
 5     1 c150_CE           137
 6     2 c4000_BCE         125
 7     2 c3300_BCE         133
 8     2 c1850_BCE         129
 9     2 c200_BCE          141
10     2 c150_CE           136
# i 140 more rows
# i Use `print(n = ...)` to see more rows
\end{outR}

\noindent
You can see that the output is telling us that the \R{\$period} column is a character vector (notice the \R{<chr>}). In other words, R does not know that \R{c4000\_BCE}, \R{c3300\_BCE}, \R{c1850\_BCE}, etc. are categories. It just sees 150 individual character values in that particular column. For the purpose of plotting and analyses, it is important that R understands that these are levels of a factor (i.e., it is important that it treats these as categories). We can easily tell R that a particular column is a factor using the function \R{factor()}.\footnote{Technically, when we use this function we are replacing an existing column with a new column that happens to be a factor. We are not really ``telling'' R it is a factor, we are ``creating'' a factor - but that's just a nitpicky semantic issue.}

\begin{inR}
skulls_tidy$period <- factor(skulls_tidy$period)

skulls_tidy
\end{inR}
\begin{outR}
# A tibble: 150 Ã— 3
   skull period    max_breadth
   <dbl> <fct>           <dbl>
 1     1 c4000_BCE         131
 2     1 c3300_BCE         124
 3     1 c1850_BCE         137
 4     1 c200_BCE          137
 5     1 c150_CE           137
 6     2 c4000_BCE         125
 7     2 c3300_BCE         133
 8     2 c1850_BCE         129
 9     2 c200_BCE          141
10     2 c150_CE           136
# i 140 more rows
# i Use `print(n = ...)` to see more rows
\end{outR}


\noindent
Notice that now the column \R{\$period} is now listed as \R{<fct>}, which stands for ``factor.''  Moreover, if we isolate the column after doing this ...

\begin{inR}
skulls_tidy$period
\end{inR}
\begin{outR}
...
[136] c4000_BCE c3300_BCE c1850_BCE c200_BCE  c150_CE  
[141] c4000_BCE c3300_BCE c1850_BCE c200_BCE  c150_CE  
[146] c4000_BCE c3300_BCE c1850_BCE c200_BCE  c150_CE  
Levels: c150_CE c1850_BCE c200_BCE c3300_BCE c4000_BCE
\end{outR}

\noindent
You can see at the bottom of the output, that the five levels of our factor have been specified. Generally speaking, to view the levels of a factor, a better practice is to use the \R{levels()} function.

\begin{inR}
levels(skulls_tidy$period)
\end{inR}
\begin{outR}
[1] "c150_CE" "c1850_BCE" "c200_BCE" "c3300_BCE" "c4000_BCE"
\end{outR}

\subsection{Ordering Levels}

Discerning readers will have noticed that the order of the levels here (from left to right) matches the order of the bars on the plot we made earlier. This is because anytime you plot or summarise categories using \textit{ggplot2} and \textit{dplyr} functions respectively, these packages silently factor the data behind the scenes and R's default behaviour is to put factors in alphabetical order (which is why we saw the order we did).  But we can change the order by specifying it inside the factor function.

\begin{inR}
mm_data$type <- factor(mm_data$type,
  levels = c("red", "orange", "yellow", "green", "blue", "brown")
)

levels(mm_data$type)
\end{inR}

\begin{outR}
[1] "red"    "orange" "yellow" "green"  "blue"   "brown" 
\end{outR}

It is important to emphasize that this does not change anything about how the data is laid out in our data frame. All those values are still in the same order. All we are doing here is telling R that, when it does it any analyses or plotting, that ``red'' comes before ``orange'' which comes before ``yellow'', and so on. For instance, when we re-run our earlier code that created the summary statistic data, you can see that the \R{\$type} column now follows this new order we have specified.

\begin{inR}
mm_summary <- mm_data |>
  group_by(type) |>
  summarise(
    m = mean(amount),
    tot_type = sum(amount),
    tot_overall = sum(mm_data$amount),
    percent = tot_type / tot_overall * 100,
    min = min(amount),
    max = max(amount)
  )

mm_summary
\end{inR}

\begin{outR}
# A tibble: 6 Ã— 7
  type       m tot_type tot_overall percent   min   max
  <fct>  <dbl>    <dbl>       <dbl>   <dbl> <dbl> <dbl>
1 red     7.75      372        2620    14.2     2    12
2 orange 11.3       544        2620    20.8     7    17
3 yellow  7.69      369        2620    14.1     2    14
4 green  10.1       483        2620    18.4     5    17
5 blue   10.0       481        2620    18.4     5    16
6 brown   7.73      371        2620    14.2     3    12
\end{outR}

\noindent

Moreover, when we now plot the data the bars will also have shifted their position accordingly.

\begin{inR}
ggplot(mm_summary, aes(x = type, y = m)) +
  geom_bar(stat = "identity")
\end{inR}

\vspace{2em}

\begin{figure}[H]
\includegraphics[scale = .75]{graphics/ch3Figs/bar_5.pdf}
\end{figure}

\subsection{Naming Levels}

On occasion, it will be useful to rename the levels of a factor. For instance, all of our levels are currently lower case, but to make them title case we could use the \R{levels()} function from earlier.

\begin{inR}
levels(mm_summary$type) <- c("Red", "Orange", "Yellow", "Green", "Blue", "Brown")

mm_summary
\end{inR}
\begin{outR}
# A tibble: 6 Ã— 7
  type       m tot_type tot_overall percent   min   max
  <fct>  <dbl>    <dbl>       <dbl>   <dbl> <dbl> <dbl>
1 Red     7.75      372        2620    14.2     2    12
2 Orange 11.3       544        2620    20.8     7    17
3 Yellow  7.69      369        2620    14.1     2    14
4 Green  10.1       483        2620    18.4     5    17
5 Blue   10.0       481        2620    18.4     5    16
6 Brown   7.73      371        2620    14.2     3    12
\end{outR}

\noindent
A corresponding change will be seen on the plot's x-axis labels as well when that is generated.

A word of warning is needed here. DO NOT confuse the \R{levels} \textit{argument} inside \R{factor()} function with the \R{levels()} \textit{function}. The \R{levels} argument is used for ordering levels. The \R{levels()} function is for re-naming levels.\footnote{At the risk of confusing readers, I feel obligated to mention that the \R{factor()} function has an additional argument \R{labels} that will allow you to change the level names. See R documentation: \R{?factor}}

{
\begin{itemize}
    \item Ordering levels: \R{factor(df\$column, levels = c(new order))}
    \item Naming levels: \R{levels(df\$column) <- c(new names)}
\end{itemize}
}

Particularly for beginners, factors are annoying to contend with, but they are vital for so many things within R and therefore a necessary evil. Consequently, it is recommended to new users that they submit and wholeheartedly embrace this wickedness. Only then will they find inner peace with factoring.
