%\chapter{The Basics of Loading and Manipulating Data}
\chapter{The Invocation and Metamorphosis of Data}

\IMFellEnglish
\lettrine[lines=5, realheight]{K}{NOWLEDGE} is power as they say, but data—data is something else entirely. It is the ghost in the machine, the thing lurking beneath the surface, waiting for you to look too close. Heed this warning: The data frame, and its accursed successor the tibble, are your most loyal servants \ldots{} and your most treacherous foes. Treat them with reverence, for a single misstep may awaken errors best left entombed.

\normalfont

Chapter 1 had stated that \glspl{data frame} are essential for keeping a host of related information stored in a well organized manner that is easy to manipulate. When printed to the console, data frames present information in a familiar spreadsheet-like structure that can be created, subset, and altered in various ways (see section \ref{sec:data_frames} for details). Moreover, in chapter 1, we saw how a data frame can be constructed by manually entering values with R code. And, for all but the smallest of data sets, this method, while simple, is both time-consuming and highly prone to error. A better strategy is to take an existing file of information and import that directly into R as a data frame or, depending on the nature of the data and what needs to be done with it, as a list, matrix, array, or table. Though, a data frame is usually going to be the optimal choice and will be the primary focus of this chapter.

Data can come in all manner of different layouts and file formats and, in this respect, R has the ability to handle pretty much any scenario that might arise. This chapter will be working under the assumption that the kind of data that you need to work with is in a conventional ``spreadsheet-style'' of format. That is to say, like the \R{msleep} data used in Chapter 2, there are sets of rows and columns, with each cell containing just a single value.

\section{Spreadsheet Software}
\label{sec:spreadsheet_soft}

Given the ubiquity of spreadsheet software, it is important to discuss its use and why R offers a preferable alternative for data analysis. Most spreadsheet applications have their own specific file type that is tailored to its unique purpose and platform. For instance, \textit{Microsoft's Excel} spreadsheet application has its own proprietary format called the \texttt{.xlsx} file format. The awful stock spreadsheet application on Macintosh computers, called \textit{Numbers}, uses the \texttt{.NUMBERS} file format. And if you use an open-source spreadsheet software like \textit{Libre Office's Calc} application, you may be familiar with the \texttt{.ods} file format.

As everyone who is reading this doubtlessly appreciates, spreadsheet applications like Microsoft's Excel, Numbers, Libre Office's Calc, etc., do more than just structure your data in a big table.  They allow you to do things like perform calculations, adjust cell colours, add images, insert comments, etc. And all of this is saved, in one form or another, as information inside the specific file associated with that software. These features make applications like Microsoft's Excel, for instance, a great tool for basic tasks like balancing the household budget.  However, for serious data analysis that requires the use of large data sets and complex or heavy calculations, this kind of software is going to be more of a hindrance than a help. Incorporating all those layers of additional functionality is going to boost file sizes, inflate load times, limit the amount of information the spreadsheet can hold, and increase the chance of a glitch occurring. Additionally, and most importantly, both the analyses and the data are all contained within the same file, which makes it very easy to irrevocably damage your original data set, often without even realizing it. The fact is, we should care about analysing our data efficiently and safely, not making it look pretty in what amounts to a fancy table, and this is one of the key benefits of using R.

From the point of view of R, a spreadsheet is just a way of displaying the raw information to be analysed and nothing more. The analysis of that information is what R does. Technically then, we should not be referring to something as a ``spreadsheet file,'' but rather a ``data file.'' The spreadsheet aspect of all of this is more about how the data is structured for our viewing as humans. However, data does not necessarily need to be viewed as a spreadsheet - it can be viewed in all kinds of different ways. It is just that a spreadsheet is usually the most convenient and intuitive way to view it and talk about it.

\section{Using an Ethical File Format}
\label{sec:ethical_file}

As noted above, there are a variety of different spreadsheet file types data could be formatted as (\texttt{.xlsx}, \texttt{.ods}, \texttt{.wks}, etc.). To remain consistent with open-science principles \parencite{UNESCO_open_sci}, best practice dictates that you work with your data in a file format that is both universally recognized across applications and will also stand the test of time in terms of compatibility.  In other words, we want to (ideally) work with a file format that has no immediate risk of becoming obsolete and can be read by multiple computers on multiple platforms without forcing the user to pay for some proprietary application. Along these lines, the most widely used and recognized format is the \texttt{.csv} file format.

\section{The .CSV Format}

``CSV'' stands for ``comma separated values.'' It gets its name from the fact that it is, quite literally, nothing more than a generic text document that uses commas to denote a tabular (spreadsheet) structure in the data.\footnote{``Tabular'' and ``spreadsheet'' mean the same thing here.} This is easiest to see with an example. The GitHub repository for this book contains a file called \R{skull\_cap\_partial\_wide.csv}. It is located in the \R{./data} directory at the following URL:

\begin{center}
\url{https://github.com/statistical-grimoire/book/blob/main/data/Egyptian-skulls}
\end{center}

\noindent

This data represents a subset of a much larger dataset\footnote{\R{Thomson\_Randall-MacIver\_1905.csv}}, containing estimated cranial capacities in cubic centimetres for 1,449 ancient Egyptian skulls.\footnote{This is not necessarily a statistic anyone should care about, but Ancient Egypt is really cool and skulls are metal AF. Also, for any Americans reading this, 1 centimetre is equal to 1.181 barleycorns.} These skulls span ten distinct historical periods, ranging from the early predynastic era to the Roman occupation of Egypt.

Upon opening the file within GitHub,\footnote{\url{https://github.com/statistical-grimoire/book/blob/main/data/Egyptian-skulls/skull_cap_partial_wide.csv}} you will see that it displays the file's contents in a fairly typical spreadsheet layout (see Table \ref{tab:skulls_wide} for an example displaying the first 10 rows).

\vspace{1em}

\input{tables/ch-3/skulls_wide.tex}

\vspace{1em}

\noindent
However, this is just how GitHub presents \texttt{.csv} files. The actual raw data is a basic text document that separates individual values with a comma. We can see this more clearly if we click the button on GitHub labelled ``Raw'' which will present the file in its unaltered (i.e., raw) text format. The first 10 rows can be seen below:

\vspace{1em}
\begin{listing}[H]
\begin{raw}
sex,predynastic,c4800BC,c4200BC,c4000BC,c3500BC,c2780BC,c1590BC,c378BC,c331BC,c3700BC
Male,1370,1410,1320,1445,1395,1425,1440,1310,1450,
Male,1250,1445,1565,1540,1420,1505,1355,1395,1460,
Male,1430,1440,1600,1565,1380,1360,1490,1360,1360,
Male,1350,1340,1460,1710,1260,1385,1425,1485,1410,
Male,1130,1460,1520,1690,1285,1350,1380,1365,1215,
Male,1670,1290,1440,1775,1505,1440,1490,1220,1320,
Male,1195,1290,1740,1390,1230,1400,1385,1195,1550,
Male,1500,1385,1410,1620,1250,1255,1270,1410,1320,
Male,1325,1290,1510,1500,1315,1450,1585,1370,1460,
Male,1480,1565,1550,1255,1360,1310,1330,1365,1560,
\end{raw}
\caption*{Example of the \R{skull\_cap\_partial\_wide.csv} data file displayed in its raw text format. Only the first ten rows are shown.}
\end{listing}

\vspace{1em}

Comparing the two versions it can readily be seen how the commas are functioning. They separate individual columns and each new line represents a new row in the spreadsheet. This not only makes it easy to read \texttt{.csv} files within a basic text editor, but create them as well. Just save (or rename) the text document with a \texttt{.csv} file extension (which you may need to configure your computer to display). Alternatively, if you have a good spreadsheet software on your computer, they will always have the ability to \textit{``Save As''} a \texttt{.csv} file or \textit{``Export''} to one. For instance, the save menu of Microsoft Excel will present the user with a drop down list of potential file types it can save as and (as of writing this) has four different versions of \texttt{.csv} files (the best option is the one labelled ``UTF-8 Comma delimited''). By contrast the Numbers application on a Mac will not permit a spreadsheet to save as anything other than a \texttt{.NUMBERS} file, but will allow you to export your saved spreadsheet as a \texttt{.csv}. Just select \textit{File $\rightarrow$ Export To $\rightarrow$ CSV.}

\section{Delimiters}

In the case of the \R{Max\_Breadth\_TRM\_1905.csv} file, the comma is functioning as a \gls{delimiter}; which is to say it is a character that defines the limits of (i.e., it ``delimits'') individual values. Commas are not the only characters that can be used to delimit, any character can technically be used. Other common delimiters include semicolons (;) and tab-key spaces. Semicolons are often used when the data is logged with commas representing decimal points instead of periods (e.g., 13.666 = 13,666), which is a frequent practice in many countries. Oddly, when a delimited file uses semicolons, it is still often given a \texttt{.csv} file extension despite it being a completely different character.  In R, to avoid confusion, the convention is to refer to these semicolon delimited files as \R{csv2} files in function names (e.g., \R{write\_csv()} would use a comma to delimit whereas \R{write\_csv2()} would use a semicolon).

Tab spaces (i.e., pressing ``tab'' on your keyboard), are also frequently employed as a delimiter, but these are usually denoted as \texttt{.tsv} files (i.e., tab separated values). In fact, the name for the keyboard key ``tab'' comes from the the verb ``tabulate'' because the key facilitated easier generation of tables when working on a typewriter. Prior to the tab key's development, the space bar had to be repeatedly pressed to advance the typewriter's carriage to align columns appropriately. 

If you were to save the \R{Max\_Breadth\_TRM\_1905} data set as a \texttt{.tsv} file and open it within a generic text editor, you would see something very similar to the following ...

\vspace{1em}
\begin{listing}[H]
\begin{raw}
sex	predynastic	c4800BC	c4200BC	c4000BC	c3500BC	c2780BC	c1590BC
Male	1370	1410	1320	1445	1395	1425	1440	1310	1450	
Male	1250	1445	1565	1540	1420	1505	1355	1395	1460	
Male	1430	1440	1600	1565	1380	1360	1490	1360	1360	
Male	1350	1340	1460	1710	1260	1385	1425	1485	1410	
Male	1130	1460	1520	1690	1285	1350	1380	1365	1215	
Male	1670	1290	1440	1775	1505	1440	1490	1220	1320	
Male	1195	1290	1740	1390	1230	1400	1385	1195	1550	
Male	1500	1385	1410	1620	1250	1255	1270	1410	1320	
Male	1325	1290	1510	1500	1315	1450	1585	1370	1460	
Male	1480	1565	1550	1255	1360	1310	1330	1365	1560	
\end{raw}
\caption*{Excerpt of the \R{skull\_cap\_partial\_wide.csv} file displayed in raw text format as if it were a \texttt{.tsv}. Only the first 10 rows are shown; the last three column headers (\texttt{c378BC}, \texttt{c331BC}, and \texttt{c3700BC}) are omitted for space.}
\end{listing}
\vspace{1em}

\noindent Notice that the tabular separation gives the file a much more grid-like aesthetic that is easier to read. Incorporating spaces into the text file can be used to further refine the alignment.

\section{Reading a CSV File into R}

Now that we have a good sense of what a \texttt{.csv} file is we should discuss how to load it into R as a data frame object so we can conduct our analyses. To begin with, you should download \R{Max\_Breadth\_TRM\_1905.csv} from the aforementioned GitHub repo by simply clicking the ``down arrow'' icon labelled ``\textit{Download raw file.}'' Once downloaded, simply place the file inside your working directory.\footnote{If you are unsure what a ``working directory'' is see section \ref{sec:dir}} Depending on the browser you are using you may have to hunt around for the download option. For instance, if you are using Safari, you may have to select ``more file actions.''

With the file in its appropriate location you can simply run the function \R{read\_csv()} and give it the full name (with extension) of your file. This will create a data frame object in R. However, \R{read\_csv()} is a function that belongs to the \textit{readr} package which is part of the \textit{tidyverse}, so if you do not have the \textit{tidyverse} loaded, this will not work. In order to easily call our loaded data, we will assign it the name \R{mm\_df}.

\begin{inR}
library(tidyverse)
skulls <- read_csv("Max_Breadth_TRM_1905.csv")
\end{inR}

\begin{outR}
Rows: 30 Columns: 6                                                                          
── Column specification ─────────────────────────────────────────────────────────────────────
Delimiter: ","
dbl (6): skull, c4000_BCE, c3300_BCE, c1850_BCE, c200_BCE, c150_CE

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{outR}

Running the above code presents us with some useful information about the data set we have loaded.  We can see that it has 30 rows and 6 columns, uses a \R{,} as a delimiter, and the 6 columns all consist of \R{dbl} values, which is a shorthand way of referring to \textit{double-precision number}. To simplify a complex story, R has multiple types of \textit{numeric} objects; i.e., it has multiple ways of representing a number. A \textit{double}, as its often referred to, is one such representation. If that is confusing, don't worry, what is important to take away from the output is that \R{dbl} means the 6 columns all contain numeric values (i.e., we can use them to do mathematics).

Running \R{skulls} will print the data frame to the console.

\begin{inR}
skulls
\end{inR}

\begin{outR}
# A tibble: 30 × 6
   skull c4000_BCE c3300_BCE c1850_BCE c200_BCE c150_CE
   <dbl>     <dbl>     <dbl>     <dbl>    <dbl>   <dbl>
 1     1       131       124       137      137     137
 2     2       125       133       129      141     136
 3     3       131       138       132      141     128
 4     4       119       148       130      135     130
 5     5       136       126       134      133     138
 6     6       138       135       140      131     126
 7     7       139       132       138      140     136
 8     8       125       133       136      139     126
 9     9       131       131       136      140     132
10    10       134       133       126      138     139
# i 20 more rows
# i Use `print(n = ...)` to see more rows
\end{outR}

You can now subset and manipulate the data frame \R{skulls} like was done in Chapter 1 when we first discussing data frames (see section \ref{sec:data_frames}). For instance, if we wanted to look at the mean breadth of skulls from the Early Dynastic Period (\textit{circa} 3300 BCE) we could simply run:

\begin{inR}
mean(skulls$c3300_BCE)
\end{inR}
\begin{outR}
[1] 132.3667
\end{outR}

\subsubsection{\texttt{read.csv()} vs. \texttt{read\_csv()}}

To load the \R{skulls} data frame above, we used the function \R{read\_csv()}, which is part of the \textit{tidyverse}. However, base R has a similar function, \R{read.csv()}, that will do essentially the same thing - it will read a \texttt{.csv} file in to R. For most use cases there is little advantage to adopting one function over the other, but if you have the \textit{tidyverse} loaded, you may as well use \R{read\_csv()} because it does have some big advantages. First, it offers excellent customization options, which are particularly useful when loading very large datasets or merging multiple datasets. Second, it alerts you to any issues encountered during the loading process. Third, it performs much faster under heavy loads than its base R counterpart, even providing a progress bar when reasonable to do so. Finally, instead of a data frame, it stores the data as a \textit{tibble}, which will be discussed later.

\subsection{Reading Other File Types into R}

If your data is delimited by some character other than a comma (e.g., a semicolon, tab, backslash, etc.), there is a more general function that can be employed called \R{read\_delim()} which allows you to specify any delimiter (i.e., separator) using the argument \R{delim}. For instance, we could have loaded the \R{skulls} data in the following way:

\begin{inR}
skulls <- read_delim("Max_Breadth_TRM_1905.csv", delim = ",")
\end{inR}

\vspace{1em}

\noindent
If your text document was separated by semicolons you would just include \R{delim = ";"}, if it was separated using tabs you would just \R{delim = "\textbackslash t"}, and so on. 

One thing that is worth appreciating about delimited files is that their file extension (e.g., the \texttt{.csv} or \texttt{.tsv} at the end of the file name) is irrelevant to how R reads the file. As has been previously emphasized, \texttt{.csv} files and \texttt{.tsv} files for instance, are just generic text documents, nothing more. This means you may see them with the file extension \texttt{.txt}, but that will not impact how any of the above functions operate.

Now, what would you do if you wanted to load a Microsoft Excel spreadsheet file (i.e., a \texttt{.xlsx} file) into R directly? Well as per the discussion on spreadsheets and ethical file formats (see section \ref{sec:spreadsheet_soft} and \ref{sec:ethical_file}), the best practice is to save it as a \texttt{.csv} using Excel and load that new file directly into R. However, should you wish to eschew this advice, the \textit{tidyverse} does have a package called \textit{readxl} with functions that will allow you to do this. This is not part of the nine core packages, so it will need to be loaded using the \R{library()} function. A word of warning is in order though. As well made as the \textit{readxl} package is, reading \texttt{.xlsx} files directly will, almost certainly, cause more problems than it solves. These files are not intended to be read by anything other than Excel and Microsoft does not want them read by anything other than Excel. Thus, by loading the \texttt{.xlsx} file directly into R, you are (computationally speaking) picking an unnecessary fight with Microsoft. Nine times out of ten, you will win that fight thanks to \textit{readxl}, but you will still probably end up with some nasty bruises and scars.

\section{Tibbles vs. Data Frames}

In the output for \R{skulls} (and the \R{msleep} data from chapter 2) you can see that the output printed to the console specifies that we are looking at something called a \gls{tibble}: \R{\# A tibble: 30 x 6}. The output also helpfully displays the dataset's dimensions and the class of object contained within each column. This is in contrast to the data frame created in chapter 1, which did not do any of that for us. In the \textit{tidyverse's} own words, a tibble is a ...

\begin{displayquote}
\headingfont
%\large
modern reimagining of the data.frame, keeping what time has proven to be effective, and throwing out what is not. Tibbles are data.frames that are lazy and surly: they do less (i.e. they don’t change variable names or types, and don’t do partial matching) and complain more (e.g. when a variable does not exist). This forces you to confront problems earlier, typically leading to cleaner, more expressive code. Tibbles also have an enhanced print( ) method which makes them easier to use with large datasets containing complex objects.

- \textit{\url{https://tibble.tidyverse.org/}}

\vspace{-1em}
(2024/07/28)

\end{displayquote}

In terms of basic usage, tibbles function almost identically to the classic data frame discussed in chapter 1. For instance, we can re-create chapter 1's data frame as a tibble using an identical syntax.

\begin{inR}
df <- tibble(
  Subject = 1:10,
  Group = c("Exp", "Cont", "Exp", "Cont", "Exp", "Exp",
            "Cont", "Exp", "Cont", "Cont"),
  Value = c(-0.36,  0.28,  1.54,  0.51, -1.28,  1.15,
            -2.22, -0.51,  NA, -1.04)
)

df
\end{inR}

\begin{outR}
# A tibble: 10 × 3
   Subject Group Value
     <int> <chr> <dbl>
 1       1 Exp   -0.36
 2       2 Cont   0.28
 3       3 Exp    1.54
 4       4 Cont   0.51
 5       5 Exp   -1.28
 6       6 Exp    1.15
 7       7 Cont  -2.22
 8       8 Exp   -0.51
 9       9 Cont  NA   
10      10 Cont  -1.04
\end{outR}

\noindent
There are a number of interesting differences between tibbles and data frames, but nothing that merits any in depth discussion for a beginner with R. For the most part they behave identically. However, there is one difference worth mentioning: for tibbles, indexing a single column by specifying row and column values outputs a tibble. For example, suppose we use our index brackets, \R{[ ]}, to isolate the first 5 rows of column 3 in the \R{skulls} data.

\begin{inR}
skulls[1:5, 3]
\end{inR}

\begin{outR}
# A tibble: 5 × 1
  c3300_BCE
      <dbl>
1       124
2       133
3       138
4       148
5       126
\end{outR}

\noindent This seems sensible enough behaviour, but is in contrast to the traditional behaviour of R's data frame which will output a vector unless more than one column is selected.

\begin{inR}
skulls_df <- read.csv("Max_Breadth_TRM_1905.csv")

skulls_df[1:5, 3]
\end{inR}

\begin{outR}
[1] 124 133 138 148 126
\end{outR}

This may seem to be a trivial distinction; however, operations such as computing the mean of a column are common and often require inserting a numeric vector. Consequently, when the output is a tibble rather than a numeric or logical vector, attempting such operations results in an error.

\begin{inR}
mean(skulls[1:5, 3])
\end{inR}

\begin{outR}
[1] NA
Warning message:
In mean.default(skulls[1:5, 3]) :
  argument is not numeric or logical: returning NA
\end{outR}

\noindent However, you can set the argument \R{drop = TRUE} inside the indexing brackets to coerce the output into a vector.

\begin{inR}
skulls[1:5, 3, drop = TRUE]
mean(skulls[1:5, 3, drop = TRUE])
\end{inR}

\begin{outR}
[1] 124 133 138 148 126
[1] 133.8
\end{outR}

Should the need arise, switching between tibbles and data frames is a simple matter. For example, to convert our tibble \R{skulls} to a data frame, we can simply use the \R{as.data.frame()} function in R.

\begin{inR}
# tibble to data frame
skulls <- as.data.frame(skulls)
skulls
\end{inR}
\begin{outR}
   skull c4000_BCE c3300_BCE c1850_BCE c200_BCE c150_CE
1      1       131       124       137      137     137
2      2       125       133       129      141     136
3      3       131       138       132      141     128
4      4       119       148       130      135     130
5      5       136       126       134      133     138
6      6       138       135       140      131     126
7      7       139       132       138      140     136
8      8       125       133       136      139     126
9      9       131       131       136      140     132
10    10       134       133       126      138     139
...
\end{outR}

\noindent
To convert it back to a tibble:
\begin{inR}
# data frame to tibble
skulls <- as_tibble(skulls)
skulls
\end{inR}
\begin{outR}
# A tibble: 30 × 6
   skull c4000_BCE c3300_BCE c1850_BCE c200_BCE c150_CE
   <dbl>     <dbl>     <dbl>     <dbl>    <dbl>   <dbl>
 1     1       131       124       137      137     137
 2     2       125       133       129      141     136
 3     3       131       138       132      141     128
 4     4       119       148       130      135     130
 5     5       136       126       134      133     138
 6     6       138       135       140      131     126
 7     7       139       132       138      140     136
 8     8       125       133       136      139     126
 9     9       131       131       136      140     132
10    10       134       133       126      138     139
# i 20 more rows
# i Use `print(n = ...)` to see more rows
\end{outR}

\subsection{Displaying Tibbles in the Console}

\subsubsection{Tibble Dimensions}

Given the limited screen space and the large size of most datasets, tibbles are designed to display only the first 10 rows when printed to the console, making it easier for users to work with their data.

Generally, if you want to view an entire data set, the best practice is not to display it in the console but rather use R's \R{View()} function which opens it in a spreadsheet-style window. That being said, many people will still find the number of rows displayed by a tibble within the console lacking, particularly if you are working on anything other than a small laptop. For this reason, the 10 row limit is a behaviour which can be circumvented in various ways. One simple way is to make use of the \R{print()} function. For instance, if we want to display the first 20 rows we can simply run ...

\begin{inR}
print(skulls, n = 20)
\end{inR}

\vspace{1em}

An alternative method is to change R's default display behaviour by setting the minimum number of rows to output using the \R{options()} function.

\begin{inR}
options(pillar.print_min = 20)
skulls
\end{inR}

\vspace{1em}

\noindent
If that method is your preference, then it is usually advisable to place the \R{options()} code at the top of your R script because it only needs to be run once.\footnote{If you are wondering why we specify \R{pillar...} to set rows, it's because \textit{pillar} is a package in the \textit{tidyverse}.}

What if you wanted to display every single row each time you print a tibble? Well, recall that R represents infinity in the positive direction as (\R{Inf}). We can use that to our advantage here:

\begin{inR}
options(pillar.print_min = Inf)
skulls
\end{inR}

\vspace{1em}

What about columns though? Well, interestingly tibbles will actually conform to the size of your console screen.  So if you can only fit five columns on screen, the tibble will only display those five and notify you of the others not displayed beneath the output. This is done to preserve the ``rectangleness'' of the data so it can be visualized appropriately. This also stands in stark contrast to how base R's data frames behave, which will stack columns on top of each other with no consideration of column or row space. Admittedly, its nice to have all that information displayed, but it comes at the cost of being difficult for a human to visually parse. That being said, if you wanted your tibbles to behave like this and always display all columns, you can just add an additional argument, \R{pillar.width = Inf}, to the \R{options()} function:

\begin{inR}
options(
  pillar.print_min = 20,
  pillar.width = Inf
)

skulls
\end{inR}

\vspace{1em}

\noindent
However, if you prefer a more temporary solution, you can just add a \R{width} argument to the \R{print()} function. E.g., 

\begin{inR}
print(skulls, n = 20, width = Inf)
\end{inR}

\vspace{1em}

\subsubsection{The Precision and Display of Decimals in Tibbles}
\label{sec:sig_digs}

To save space and facilitate easier reading, both tibbles and data frames will round values with many decimal values. Though, in the case of tibbles, they do not just simply round to a preset number of digits.  To illustrate what tibbles are doing in this respect, recall that R has a built-in constant for $\pi$.

\begin{inR}
pi
\end{inR}
\begin{outR}
[1] 3.141593
\end{outR}

\noindent
Using that, we will create a simple data frame that repeats $\pi$ four times within a single column.

\begin{inR}
pi_df <- tibble(pie = rep(pi, 4))
pi_df
\end{inR}
\begin{outR}
# A tibble: 10 × 1
     pie
   <dbl>
 1  3.14
 2  3.14
 3  3.14
 4  3.14
\end{outR}

\noindent
One thing that will be noticed is that the tibble is only displaying $\pi$ to two decimal places. However, all of the digits still exist in R's memory and any calculations you do will take those unseen digits into account. For instance, if we isolate the first row's value you can see that all the digits of $\pi$ are displayed.

\begin{inR}
print(pi_df$pie[1], digits = 16)
\end{inR}
\begin{outR}
[1] 3.141592653589793
\end{outR}

\noindent
It is important to understand that tibbles do not strictly control the handling of decimals. Instead, they work with \gls{significant figures}, also known as \textit{significant digits}.  This allows the tibble to preserve its desired ``rectangleness,'' giving it cleaner looking columns. Since tibbles default to three significant figures, $\pi$ will only display as 3.14. 

As a refresher of primary school math, with significant figures, everything in front of the decimal point is always displayed, but each number in front of that decimal point uses up a significant figure (a.k.a. a ``sig fig'' or ``sig dig''). For instance, if you had a number like 666.13.  Displaying that to two sig figs would give you 666.  Displayed to three sig figs would again be 666. Displayed to four sig figs would be 666.1. Five sig digs would be 666.13. Six sig figs would be 666.130. Seven would be 666.1300, and so on.

To increase the number of sig figs shown within a tibble we can simply add another argument to the \R{options()} function:

\begin{inR}
options(pillar.sigfig = 16)
pi_df
\end{inR}
\begin{outR}
# A tibble: 10 × 1
                   pie
                 <dbl>
 1 3.141592653589793e0
 2 3.141592653589793e0
 3 3.141592653589793e0
 4 3.141592653589793e0
\end{outR}

Because of limitations of 64-bit computing, a tibble is not going let you exceed 16 sig figs and in certain cases will display results in scientific notation. In this case we see some scientific notation, but it is to the power of 0, so it can be ignored.

\section{Wide Data vs. Tidy Data}

\subsection{Wide Data}
\label{sec:wide_data}

Examining the \R{Max\_Breadth\_TRM\_1905.csv} file loaded at the beginning of this chapter, we see that the data is structured logically. Each column represents skull measurements from a distinct period in Egypt’s history. By examining the rows (or the values provided in column 1), we can see that each period includes measurements from 30 skulls. Thus, with five periods, this totals 150 measured skulls.

\begin{inR}
skulls
\end{inR}

\begin{outR}
# A tibble: 30 × 6
   skull c4000_BCE c3300_BCE c1850_BCE c200_BCE c150_CE
   <dbl>     <dbl>     <dbl>     <dbl>    <dbl>   <dbl>
 1     1       131       124       137      137     137
 2     2       125       133       129      141     136
 3     3       131       138       132      141     128
 4     4       119       148       130      135     130
 5     5       136       126       134      133     138
 6     6       138       135       140      131     126
 7     7       139       132       138      140     136
 8     8       125       133       136      139     126
 9     9       131       131       136      140     132
10    10       134       133       126      138     139
# i 20 more rows
# i Use `print(n = ...)` to see more rows
\end{outR}

This style of layout makes it easy to do certain things with the data. For instance, if we wanted to know the mean breadth of skulls \textit{circa} 4000 BCE, we could just run 

\begin{inR}
mean(skulls$c4000_BCE)
\end{inR}

\begin{outR}
[1] 131.3667
\end{outR}

If we wanted to obtain the mean of every column, we could use the \R{apply()} function. This function literally \textit{applies} a function of your choosing to either the columns or rows. So we could, for instance, apply the \R{mean()} function to each \textit{column}.

\begin{inR}
apply(skulls, MARGIN = 2, FUN = mean)
\end{inR}

\begin{outR}
    skull c4000_BCE c3300_BCE c1850_BCE  c200_BCE   c150_CE 
  15.5000  131.3667  132.3667  134.4667  135.5000  136.1667 
\end{outR}

\noindent
The argument \R{FUN} specifies what function is applied. The argument \R{MARGIN} specifies whether that function is applied to the rows (\R{1}) or columns (\R{2}). In this case we are applying it to columns, so we specified \R{MARGIN = 2}.

If you wanted to know the sum of each row ignoring the first column \R{\$skull}, you could \textit{apply} the \R{sum()} function to the \textit{rows}. We can use what we learned about indexing in chapter 1 to ignore that first column (see section \ref{sec:df_Index}).

\begin{inR}
apply(skulls[, 2:6], MARGIN = 1, FUN = sum)
\end{inR}

\begin{outR}
 [1] 666 664 670 662 667 670 685 659 670 670 674 677 663 677
[15] 678 670 671 655 676 678 672 669 691 651 665 678 656 666
[29] 683 663
\end{outR}

At first glance, R makes it fairly easy to work with data in this format. However, we have not attempted anything particularly complex yet, and all our columns are conveniently numeric. In reality, this layout—referred to in this book as \gls{wide data}—is not ideal for most types of analysis or visualization. That may not be immediately obvious, and until you attempt more advanced analyses, you might not be fully convinced of the truth of that. For now, you will just have to take that point on faith. 

\textit{Wide data} spreads variables across multiple columns. In this case, we can treat of the different historical periods of the skulls (c4000\_BCE, c3300\_BCE, etc.) as a single variable in its own right. We might call this variable simply ``period.'' That is to say, c4000\_BCE is a \textit{period}, c3300\_BCE is a \textit{period}, c1850\_BCE is a \textit{period}, and so on. There is also a second variable, which we could refer to as the ``maximum breadth'' of the skulls.

When organizing or arranging data, best practices dictate that you \textbf{restrict a single variable to a single column}. In this case, the variable ``period'' is being spread across five columns. And the variable ``maximum breadth'' is found within each of those five columns. To fix this, we can use the \textit{tidyverse} function \R{pivot\_longer()}.

\begin{inR}
skulls_tidy <- pivot_longer(skulls,
  cols = c4000_BCE:c150_CE,
  names_to = "period",
  values_to = "max_breadth"
)

skulls_tidy
\end{inR}
\begin{outR}
# A tibble: 150 × 3
   skull period    max_breadth
   <dbl> <chr>           <dbl>
 1     1 c4000_BCE         131
 2     1 c3300_BCE         124
 3     1 c1850_BCE         137
 4     1 c200_BCE          137
 5     1 c150_CE           137
 6     2 c4000_BCE         125
 7     2 c3300_BCE         133
 8     2 c1850_BCE         129
 9     2 c200_BCE          141
10     2 c150_CE           136
# i 140 more rows
# i Use `print(n = ...)` to see more rows
\end{outR}

Notice a few differences with this pivoted data. There are now considerably more rows in the data, 150 vs. 30, and there are two new columns, \R{\$period} and \R{\$max\_breadth} which have replaced the five different time period columns. In terms of the \R{pivot\_longer()} function we used, the most important argument we specified is \R{cols}, as this defines which columns are going to be collapsed into a single column. The arguments \R{names\_to} and \R{values\_to} just specify the name of the new columns and are not strictly required, but are good practice to include.

In the above code, we used \R{:} to specify a range of columns (e.g., from c4000\_BCE to c150\_CE), but we could have instead provided a vector of column names, as shown below:

\begin{inR}
cols = c(c4000_BCE, c3300_BCE, c1850_BCE, c200_BCE, c150_CE)
\end{inR}
\vspace{1em}

\noindent It is worth noting that the \textit{tidyverse} has numerous methods for selecting multiple columns that do not exist as part of base R. For a rundown of each see the R documentation: \R{?tidyr\_tidy\_select}. Additionally, if you are operating outside of the \textit{tidyverse}, analogous base R functions, will usually require column names to be entered as character strings. E.g.,

\begin{inR}
cols = c("c4000_BCE", "c3300_BCE", "c1850_BCE", "c200_BCE", "c150_CE")
\end{inR}
\vspace{1em}

\subsection{Tidy data}

By collapsing the six colour columns into one, the data has ascended into a sacred arrangement known as \gls{tidy data} (or, as some heretics call it, ``the long format'').  Tidy data is a cornerstone of the \textit{tidyverse's} thaumaturgy and all tidy data adheres to three basic precepts:

\begin{minipage}{\textwidth}

\begin{enumerate}[label=\Roman*.]
\IMFellEnglish
    %\setlength\itemsep{-1em}
    %\large
    \item Each variable is a column; each column is a variable.
    \item Each observation is a row; each row is an observation.
    \item Each value is a cell; each cell is a single value.
\end{enumerate}
\end{minipage}

\noindent
It can be seen that the M\&M data now satisfies these three standards, as did the \R{msleep} data used in chapter 2. As we progress through the remainder of this chapter, it will become apparent that having your data in this tidy form will greatly facilitate both plotting and analysis. 


\section{Laying Pipe (The \texttt{|>} and \texttt{\%>\%} Operators)}

One of the most significant contributions of the \textit{tidyverse} to R has been its seamless implementation of what is known as `piping' syntax, which allows for an almost otherworldly level of efficiency. However, this is not to say that piping was a concept invented entirely by the \textit{tidyverse}. Rather, the \textit{tidyverse}'s consistent and innovative use of it brought its potential to light, leading to widespread adoption within the R community. The best evidence of this influence is the integration of piping into base R as of \href{https://stat.ethz.ch/pipermail/r-announce/2021/000670.html}{version 4.1.0}, released in 2021.\footnote{Although I have no direct evidence that the \textit{tidyverse} directly motivated the addition of the pipe operator to base R, it seems unlikely to be a coincidence given the ubiquity of \R{\%>\%} and the popularity of the \textit{dplyr} package.} But what exactly is `piping'?

The essence of piping is that you are transferring the output of one thing to another. For instance, suppose we wanted to know the mean maximum breadth across all periods of skulls. We could of course insert the \R{\$max\_breadth} column into the mean function like so ...

\begin{inR}
mean(skulls_tidy$max_breadth)
\end{inR}
\begin{outR}
[1] 133.9733
\end{outR}

Alternatively, we could ``pipe'' (i.e., transfer) the \R{\$max\_breadth} column in our tidy data to the \R{mean()} function using R's pipe operator \R{|>}

\begin{inR}
skulls_tidy$max_breadth |> mean()
\end{inR}
\begin{outR}
[1] 133.9733
\end{outR}

\noindent We could then send that output to something like the \R{round()} function.

\begin{inR}
skulls_tidy$max_breadth |>
    mean() |> 
    round(2)
\end{inR}
\begin{outR}
[1] 133.97
\end{outR}

As another example, suppose we wanted a tidy data frame that only contained skulls from the Middle Kingdom (circa 1850 BCE). The standard methodology would be to specify the data frame within the \R{filter()} function, like so ...

\begin{inR}
filter(skulls_tidy, period == "c1850_BCE")
\end{inR}
\begin{outR}
# A tibble: 30 × 3
   skull period    max_breadth
   <dbl> <chr>           <dbl>
 1     1 c1850_BCE         137
 2     2 c1850_BCE         129
 3     3 c1850_BCE         132
 4     4 c1850_BCE         130
 5     5 c1850_BCE         134
 6     6 c1850_BCE         140
 7     7 c1850_BCE         138
 8     8 c1850_BCE         136
 9     9 c1850_BCE         136
10    10 c1850_BCE         126
# i 20 more rows
# i Use `print(n = ...)` to see more rows
\end{outR}

\noindent
Alternatively, we could pipe the data into the \R{filter()} function.

\begin{inR}
skulls_tidy |> filter(period == "c1850_BCE")
\end{inR}
\begin{outR}
# A tibble: 30 × 3
   skull period    max_breadth
   <dbl> <chr>           <dbl>
 1     1 c1850_BCE         137
 2     2 c1850_BCE         129
 3     3 c1850_BCE         132
 4     4 c1850_BCE         130
 5     5 c1850_BCE         134
 6     6 c1850_BCE         140
 7     7 c1850_BCE         138
 8     8 c1850_BCE         136
 9     9 c1850_BCE         136
10    10 c1850_BCE         126
# i 20 more rows
# i Use `print(n = ...)` to see more rows
\end{outR}

In addition to this, suppose you did not want the \R{\$skull} column in the output. To achieve this, this output could be further piped into the \textit{tidyverse's} \R{select()} function which allows you to grab specific columns.

\begin{inR}
skulls_tidy |> 
  filter(period == "c1850_BCE") |> 
  select(period, max_breadth)
\end{inR}
\begin{outR}
# A tibble: 30 × 2
   period    max_breadth
   <chr>           <dbl>
 1 c1850_BCE         137
 2 c1850_BCE         129
 3 c1850_BCE         132
 4 c1850_BCE         130
 5 c1850_BCE         134
 6 c1850_BCE         140
 7 c1850_BCE         138
 8 c1850_BCE         136
 9 c1850_BCE         136
10 c1850_BCE         126
# i 20 more rows
# i Use `print(n = ...)` to see more rows
\end{outR}

Now that the logic of piping is clear, it is worth reiterating that the \R{|>} operator is a relatively new arrival in base R. Prior to this, the convention would be to use the \textit{tidyverse's} pipe operator \R{\%>\%} instead. This comes from a package called \textit{magrittr}, which contains a variety of pipes for different purposes, but the most significant of these is \R{\%>\%}. Before R version  4.1.0, \R{\%>\%} was the de facto pipe used by the R community at large. However, the recommended wisdom now (even by the keepers of the \textit{tidyverse}) is to use base R's pipe and not \textit{magrittr's}. That being said, many are unaware of this update to base R and much of the help documentation on websites like \href{https://stackoverflow.com/}{stack overflow} still use \textit{magrittr's} \R{\%>\%}. In terms of functionality, there is little meaningful difference between \R{|>} and \R{\%>\%} and all of the above code could have been written using \R{\%>\%}.

The above examples nicely show how the pipe operator works, but we should consider a more realistic use case to illustrate its versatility. 

\subsection{Data Manipulation Example}

\subsubsection{Summarising the Data}

The \R{Max\_Breadth\_TRM\_1905.csv} data we loaded earlier was of course in the wide format originally, which is rarely needed. So what we could have done instead is loaded that data in to R using \R{read\_csv}, then pipe it to the \R{pivot\_longer()} function and, if we wanted to, pipe that to the \R{select()} function to avoid keeping irrelevant columns.

\begin{inR}
skulls <- read_csv("Max_Breadth_TRM_1905.csv") |>
  pivot_longer(
    cols = c4000_BCE:c150_CE,
    names_to = "period",
    values_to = "max_breadth"
  ) |>
  select(period, max_breadth)

skulls
\end{inR}
\begin{outR}
# A tibble: 150 × 2
   period    max_breadth
   <chr>           <dbl>
 1 c4000_BCE         131
 2 c3300_BCE         124
 3 c1850_BCE         137
 4 c200_BCE          137
 5 c150_CE           137
 6 c4000_BCE         125
 7 c3300_BCE         133
 8 c1850_BCE         129
 9 c200_BCE          141
10 c150_CE           136
# i 140 more rows
# i Use `print(n = ...)` to see more rows
\end{outR}

It is worth emphasizing the utility of the pipe operator here: It allowed us to get our data into the form we wanted without creating and calling multiple different objects in memory. Only one object was created, \R{skulls}. Moreover, the ``arrow-like'' notation of the pipe \R{|>} nicely shows the workflow, i.e., logic, of our code.

Now suppose we wanted to compute some summary statistics for this data set.  For instance, maybe we want to know the mean maximum breadth of each period. This is where the \textit{tidyverse's} functions \R{group\_by()} and \R{summarise()} become extremely useful. Both of these functions, as well as the \R{filter()} and \R{select()} functions we have been using, come from a very influential \textit{tidyverse} package called \textit{dplyr}. 

\begin{mdframed}[nobreak = true, style = miscFrame, frametitle = \Large\IMFellEnglish Box 3.1: Why is it called \textit{dplyr}?]
\color{darkgray}
\IMFellEnglish

Generally, the names of R packages are relatively intuitive or are based on an initialism of some kind. The \textit{dplyr} package is an exception to that. The package's strange name is a reference to both pliers (the tool) and a family of functions based around the \R{apply()} function that we briefly used in section \ref{sec:wide_data}. The \enquote{d} refers to data frames. i.e., it is as if you are taking a pair of pliers to data frames.

A common go-to strategy of programmers generally is to use for-loops to do much of the computational grunt work. For-loops just repeatedly execute a set of code until some condition has been satisfied. While for-loops can be used in R, its users often prefer to take a different, more efficient, \enquote{vectorized} approach. The goal is to use what are called \glspl{functional}. These are functions that accept another function as an input and produce a vector as output. That is precisely what the \R{apply()} function and its relatives like \R{lapply}, \R{sapply}, \R{vapply} do. R is incredibly adept at working with vectors, matrices, and arrays, and \textit{dplyr's} functions are all based around a strategy of using functionals for data manipulation.
\end{mdframed}

We will begin with the \R{summarise()} function which is used to create a data frame based on columns/variables in your data.

\begin{inR}
skulls |> 
  summarise(m = mean(max_breadth))
\end{inR}

\begin{outR}
# A tibble: 1 × 1
      m
  <dbl>
1  134.
\end{outR}

The code we have written is telling the \R{summarise()} function to apply the \R{mean()} function to the \R{\$max\_breadth} column. When it did this, it also created a new data frame and stored that calculation as a column called \R{\$m} (though, we could have named the column whatever we wanted).

At present, none of this may seem terribly useful; however, we can make it more useful by including the \R{group\_by()} function which will tell R to literally ``group by'' categories found in a different column or set of columns. Specifically, we can tell it to group by \R{\$period} and then summarise the data.

\begin{inR}
skulls |> 
  group_by(period) |> 
  summarise(m = mean(max_breadth))
\end{inR}
\begin{outR}
# A tibble: 5 × 2
  period        m
  <chr>     <dbl>
1 c150_CE    136.
2 c1850_BCE  134.
3 c200_BCE   136.
4 c3300_BCE  132.
5 c4000_BCE  131.
\end{outR}

We can now see the mean of each period in the data set and if we wanted, we could create another column, \R{\$n}, showing how many skulls there are in each period total by using the \R{length()} function to count the skulls.

\begin{inR}
skulls |> 
  group_by(period) |> 
  summarise(
    m = mean(max_breadth),
    n = length(max_breadth)
    )
\end{inR}
\begin{outR}
# A tibble: 5 × 3
  period        m     n
  <chr>     <dbl> <int>
1 c150_CE    136.    30
2 c1850_BCE  134.    30
3 c200_BCE   136.    30
4 c3300_BCE  132.    30
5 c4000_BCE  131.    30
\end{outR}

If we wanted to add in a column, \R{\$N}, that represented the total amount of skulls across all the periods we could count the number of rows in \R{\$skulls}...

\begin{inR}
nrow(skulls)
\end{inR}
\begin{outR}
[1] 150
\end{outR}

\noindent
... and include that in the \R{summarise()} function.

\begin{inR}
skulls |> 
  group_by(period) |> 
  summarise(
    m = mean(max_breadth),
    n = length(max_breadth),
    N = nrow(skulls)
    )
\end{inR}
\begin{outR}
# A tibble: 5 × 4
  period        m     n     N
  <chr>     <dbl> <int> <int>
1 c150_CE    136.    30   150
2 c1850_BCE  134.    30   150
3 c200_BCE   136.    30   150
4 c3300_BCE  132.    30   150
5 c4000_BCE  131.    30   150
\end{outR}

Mathematical expressions can also be applied to columns created within the \R{summarise()} function. For example, the mean maximum breadth is calculated in millimetres, but it can be easily converted to another unit, such as centimetres, by referencing the column and performing a simple mathematical operation.

\begin{inR}
skulls |> 
  group_by(period) |> 
  summarise(
    m = mean(max_breadth),
    n = length(max_breadth),
    N = nrow(skulls),
    m_cm = m / 10
    )
\end{inR}

\begin{outR}
# A tibble: 5 × 5
  period        m     n     N  m_cm
  <chr>     <dbl> <int> <int> <dbl>
1 c150_CE    136.    30   150  13.6
2 c1850_BCE  134.    30   150  13.4
3 c200_BCE   136.    30   150  13.6
4 c3300_BCE  132.    30   150  13.2
5 c4000_BCE  131.    30   150  13.1
\end{outR}


To finish up, lets include the maximum and minimum size found in each period. We will also store this as an tibble called \R{skull\_summary}.

\begin{inR}
skull_summary <- skulls |>
  group_by(period) |>
  summarise(
    m = mean(max_breadth),
    n = length(max_breadth),
    N = nrow(skulls),
    m_cm = m / 10,
    min = min(max_breadth),
    max = max(max_breadth)
  )\textbf{}

skull_summary
\end{inR}

\begin{outR}
# A tibble: 5 × 7
  period        m     n     N  m_cm   min   max
  <chr>     <dbl> <int> <int> <dbl> <dbl> <dbl>
1 c150_CE    136.    30   150  13.6   126   147
2 c1850_BCE  134.    30   150  13.4   126   140
3 c200_BCE   136.    30   150  13.6   129   144
4 c3300_BCE  132.    30   150  13.2   123   148
5 c4000_BCE  131.    30   150  13.1   119   141
\end{outR}

\subsubsection{Plotting the Summarised Data}

Now that we have neatly organized these summary statistics in a tibble, we can visualize them. Since the \R{\$period} column contains five discrete categories, a bar plot is a natural choice for representing these data, so that is what we will create.

The basic logic of plotting has been discussed at length in chapter 2, and this discussion will follow from that.\footnote{In other words, if you haven't read chapter 2, go back and do that.} The first step will be to give \textit{ggplot2} the data and tell it which columns to map to the x and y axis respectively. Then we will add the \R{geom\_bar()} function to this. In this case, we are going to display the mean (i.e., column \R{\$m}) on the y-axis because that is a fairly standard practice many people will be familiar with. Though, it is worth remembering that any of the other numeric columns could be used as well.

\begin{inR}
ggplot(skull_summary, aes(x = period, y = m)) +
  geom_bar(stat = "identity")
\end{inR}

\vspace{2em}

\begin{figure}[H]
\includegraphics[scale = .75]{graphics/ch3Figs/bar_1.pdf}
\end{figure}

The argument \R{stat = "identity"} is simply telling \textit{ggplot2} to use the values within the \R{skull\_summary} tibble to create the bars.  We needed to specify this because \textit{ggplot2} has the ability to take the raw data directly (e.g., \R{skulls}) and perform its own summary calculations. However, we do not need it to do that in this particular case, hence why we included this argument.

The resulting bar graph displays the mean maximum breadth for each period. To enhance its visual appeal, we can adjust the fill colour of the bars to reflect the corresponding time periods more effectively (see Table \ref{tab:egypt_colour}).\footnote{Technically, this is something we should NOT do because, for the sake of comparison, its better to give all the bars the same ``visual weight.'' Keeping all the bars the same colour does precisely that. Moreover, with the x-axis labels, there is no reason to add additional elements that could be distracting. That being said, if you are collaborating on a project, your collaborators will probably demand to see colourful bars irrespective this rationale (experience has taught me this). And if they outnumber you, they can probably beat you in a fight - it doesn't matter if you have the moral or logical high ground.} When we do this, we have to be mindful of the fact that the x-axis contains a \textit{discrete} scale, not a \textit{continuous} one like we saw in chapter 2 (for more information on discrete vs. continuous scales see section \ref{sec:pos_scale}).

\input{tables/ch-3/egypt_colour.tex}

First we will define our colour palette.

\begin{inR}
egypt_pal <- c("#E3C9A8", "#0C2C84", "#D4AF37", "#20603D", "#A23E0E")
\end{inR}

\vspace{1em}

Once \R{egypt\_pal} has been created we can use it to adjust the colour of the bar graph.

\begin{inR}
ggplot(skull_summary, aes(x = period, y = m)) +
  geom_bar(
    stat = "identity",
    colour = "black",
    aes(fill = period)
  ) +
  scale_fill_discrete(type = egypt_pal)
\end{inR}

\vspace{2em}

\begin{figure}[H]
\includegraphics[scale = .75]{graphics/ch3Figs/bar_2.pdf}
\end{figure}

Now, in addition to the mean maximum breadth of each period, \R{skull\_summary} also has information pertaining to the smallest and largest measured skull (these are columns \R{\$min} and \R{\$max} respectively). We could incorporate that information in our graph with the use of \glspl{error bar}. Error bars are a visual representation of our data's \textit{spread}, and the difference between the minimum and maximum represent a classic measure of spread called the \textit{range}.\footnote{If that isn't entirely clear, don't worry. The concept of spread as a statistical term will be explained in more detail in later chapters.}

To create error bars, we can simply use \textit{ggplot2's} \R{geom\_errorbar()} function. We just need to tell it which column corresponds to the bottom of the error bar (\R{ymin}) and which column corresponds to the top of the errorbar (\R{ymax}).

\begin{inR}
ggplot(mm_summary, aes(x = type, y = m)) +
  geom_bar(
    stat = "identity",
    colour = "black",
    aes(fill = type)
  ) +
  scale_fill_discrete(type = mm_palette) +
  geom_errorbar(aes(ymin = min, ymax = max), width = 0.25)
\end{inR}

\vspace{2em}

\begin{figure}[H]
\includegraphics[scale = .75]{graphics/ch3Figs/bar_3.pdf}
\end{figure}

All that remains is to update the plot’s labelling. Specifically, we should provide clearer titles for the $x$- and $y$-axes and remove the underscore (\_) in the $x$-axis labels. The legend can also be removed as it is redundant with the labelling.

To change the current labels ``c150\_CE'', ``c1850\_BCE'', ``c200\_BCE'' and so on, we can use the function \R{scale\_x\_discrete()} and use its \R{labels} argument. We just have to give it a character vector containing the new labelling in the correct order.

To remove the legend, there are different methods you could employ. In this case, since we only have the fill aesthetic mapped, it is easy enough to just add \R{guide = "none"} to the \R{scale\_fill\_discrete()} function.

\begin{inR}
ggplot(skull_summary, aes(x = period, y = m)) +
  geom_bar(
    stat = "identity",
    colour = "black",
    aes(fill = period)
  ) +
  scale_fill_discrete(type = egypt_pal, guide = "none") +
  geom_errorbar(aes(ymin = min, ymax = max), width = 0.25) +
  scale_x_discrete(
    labels = c("c150 CE", "c1850 BCE", "c200 BCE", "c3300 BCE", "c4000 BCE")
  ) +
  xlab("Period") +
  ylab("Maximum Breadth (mm)")
\end{inR}

\vspace{2em}

\begin{figure}[H]
\includegraphics[scale = .75]{graphics/ch3Figs/bar_4.pdf}
\end{figure}

After reading Chapter 2 and this chapter, one key aspect of plotting remains unaddressed: how to adjust the order of categories. Currently, the time periods are not arranged chronologically (left to right) as one might expect. How can we fix this? This is where the concept of factors becomes important.

\section{Factors}

In statistics we often refer to a categorical variable as a \gls{factor},\footnote{Independent, manipulated, and predictor variables are often given this label when they have a fixed set of categories or are continuous but only take on a limited number of discrete values. If this is unclear now, don't worry — variable types will be explained in more detail in a later chapter.} and factors have different \glspl{level}.  For instance, in our tidy data (\R{skulls\_tidy}) the variable \R{\$period} could be considered a factor. The individual periods (e.g., c4000 BCE, c3300 BCE, etc.) in that column are each a level of that factor: c4000 BCE is its own level, c3300 BCE is its own level, c1850 BCE is its own level, and so on. In other words, in the \R{skulls\_tidy} data, the factor named ``period'' has 5 levels.

To summarise, you can treat the term ``factor'' as synonymous with the terms ``column'' or ``variable.'' And you can treat the term ``level'' as synonymous with the term ``category.'' Though, this only applies to tidy data, not wide data.
{
\begin{itemize}
  \setlength\itemsep{-1em}
    \item Factor = column / variable
    \item Level = category within a column / variable
\end{itemize}
}

If we examine \R{skulls\_tidy}:

\begin{inR}
mm_data
\end{inR}
\begin{outR}
# A tibble: 150 × 3
   skull period    max_breadth
   <dbl> <chr>           <dbl>
 1     1 c4000_BCE         131
 2     1 c3300_BCE         124
 3     1 c1850_BCE         137
 4     1 c200_BCE          137
 5     1 c150_CE           137
 6     2 c4000_BCE         125
 7     2 c3300_BCE         133
 8     2 c1850_BCE         129
 9     2 c200_BCE          141
10     2 c150_CE           136
# i 140 more rows
# i Use `print(n = ...)` to see more rows
\end{outR}

\noindent
You can see that the output is telling us that the \R{\$period} column is a character vector (notice the \R{<chr>}). In other words, R does not know that \R{c4000\_BCE}, \R{c3300\_BCE}, \R{c1850\_BCE}, etc. are categories. It just sees 150 individual character values in that particular column. For the purpose of plotting and analyses, it is important that R understands that these are levels of a factor (i.e., it is important that it treats these as categories). We can easily tell R that a particular column is a factor using the function \R{factor()}.\footnote{Technically, when we use this function we are replacing an existing column with a new column that happens to be a factor. We are not really ``telling'' R it is a factor, we are ``creating'' a factor - but that's just a nitpicky semantic issue.}

\begin{inR}
skulls_tidy$period <- factor(skulls_tidy$period)

skulls_tidy
\end{inR}
\begin{outR}
# A tibble: 150 × 3
   skull period    max_breadth
   <dbl> <fct>           <dbl>
 1     1 c4000_BCE         131
 2     1 c3300_BCE         124
 3     1 c1850_BCE         137
 4     1 c200_BCE          137
 5     1 c150_CE           137
 6     2 c4000_BCE         125
 7     2 c3300_BCE         133
 8     2 c1850_BCE         129
 9     2 c200_BCE          141
10     2 c150_CE           136
# i 140 more rows
# i Use `print(n = ...)` to see more rows
\end{outR}


\noindent
Notice that now the column \R{\$period} is now listed as \R{<fct>}, which stands for ``factor.''  Moreover, if we isolate the column after doing this ...

\begin{inR}
skulls_tidy$period
\end{inR}
\begin{outR}
...
[136] c4000_BCE c3300_BCE c1850_BCE c200_BCE  c150_CE  
[141] c4000_BCE c3300_BCE c1850_BCE c200_BCE  c150_CE  
[146] c4000_BCE c3300_BCE c1850_BCE c200_BCE  c150_CE  
Levels: c150_CE c1850_BCE c200_BCE c3300_BCE c4000_BCE
\end{outR}

\noindent
You can see at the bottom of the output, that the five levels of our factor have been specified. Generally speaking, to view the levels of a factor, a better practice is to use the \R{levels()} function.

\begin{inR}
levels(skulls_tidy$period)
\end{inR}
\begin{outR}
[1] "c150_CE" "c1850_BCE" "c200_BCE" "c3300_BCE" "c4000_BCE"
\end{outR}

\subsection{Ordering Levels}

Discerning readers will have noticed that the order of the levels here (from left to right) matches the order of the bars on the plot we made earlier. This is because anytime you plot or summarise categories using \textit{ggplot2} and \textit{dplyr} functions respectively, these packages silently factor the data behind the scenes and R's default behaviour is to put factors in alphabetical order (which is why we saw the order we did).  But we can change the order by specifying it inside the factor function.

\begin{inR}
mm_data$type <- factor(mm_data$type,
  levels = c("red", "orange", "yellow", "green", "blue", "brown")
)

levels(mm_data$type)
\end{inR}

\begin{outR}
[1] "red"    "orange" "yellow" "green"  "blue"   "brown" 
\end{outR}

It is important to emphasize that this does not change anything about how the data is laid out in our data frame. All those values are still in the same order. All we are doing here is telling R that, when it does it any analyses or plotting, that ``red'' comes before ``orange'' which comes before ``yellow'', and so on. For instance, when we re-run our earlier code that created the summary statistic data, you can see that the \R{\$type} column now follows this new order we have specified.

\begin{inR}
mm_summary <- mm_data |>
  group_by(type) |>
  summarise(
    m = mean(amount),
    tot_type = sum(amount),
    tot_overall = sum(mm_data$amount),
    percent = tot_type / tot_overall * 100,
    min = min(amount),
    max = max(amount)
  )

mm_summary
\end{inR}

\begin{outR}
# A tibble: 6 × 7
  type       m tot_type tot_overall percent   min   max
  <fct>  <dbl>    <dbl>       <dbl>   <dbl> <dbl> <dbl>
1 red     7.75      372        2620    14.2     2    12
2 orange 11.3       544        2620    20.8     7    17
3 yellow  7.69      369        2620    14.1     2    14
4 green  10.1       483        2620    18.4     5    17
5 blue   10.0       481        2620    18.4     5    16
6 brown   7.73      371        2620    14.2     3    12
\end{outR}

\noindent

Moreover, when we now plot the data the bars will also have shifted their position accordingly.

\begin{inR}
ggplot(mm_summary, aes(x = type, y = m)) +
  geom_bar(stat = "identity")
\end{inR}

\vspace{2em}

\begin{figure}[H]
\includegraphics[scale = .75]{graphics/ch3Figs/bar_5.pdf}
\end{figure}

\subsection{Naming Levels}

On occasion, it will be useful to rename the levels of a factor. For instance, all of our levels are currently lower case, but to make them title case we could use the \R{levels()} function from earlier.

\begin{inR}
levels(mm_summary$type) <- c("Red", "Orange", "Yellow", "Green", "Blue", "Brown")

mm_summary
\end{inR}
\begin{outR}
# A tibble: 6 × 7
  type       m tot_type tot_overall percent   min   max
  <fct>  <dbl>    <dbl>       <dbl>   <dbl> <dbl> <dbl>
1 Red     7.75      372        2620    14.2     2    12
2 Orange 11.3       544        2620    20.8     7    17
3 Yellow  7.69      369        2620    14.1     2    14
4 Green  10.1       483        2620    18.4     5    17
5 Blue   10.0       481        2620    18.4     5    16
6 Brown   7.73      371        2620    14.2     3    12
\end{outR}

\noindent
A corresponding change will be seen on the plot's x-axis labels as well when that is generated.

A word of warning is needed here. DO NOT confuse the \R{levels} \textit{argument} inside \R{factor()} function with the \R{levels()} \textit{function}. The \R{levels} argument is used for ordering levels. The \R{levels()} function is for re-naming levels.\footnote{At the risk of confusing readers, I feel obligated to mention that the \R{factor()} function has an additional argument \R{labels} that will allow you to change the level names. See R documentation: \R{?factor}}

{
\begin{itemize}
    \item Ordering levels: \R{factor(df\$column, levels = c(new order))}
    \item Naming levels: \R{levels(df\$column) <- c(new names)}
\end{itemize}
}

Particularly for beginners, factors are annoying to contend with, but they are vital for so many things within R and therefore a necessary evil. Consequently, it is recommended to new users that they submit and wholeheartedly embrace this wickedness. Only then will they find inner peace with factoring.
